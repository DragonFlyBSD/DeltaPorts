#!/usr/bin/env python3
"""
agent-queue-runner: Process dsynth failure jobs via opencode serve.

Usage:
    agent-queue-runner --queue-root <path> [--once] [--dry-run]

Environment variables:
    OPENCODE_URL              (required) e.g., http://192.168.1.10:4096
    OPENCODE_PROVIDER         (optional) e.g., opencode (default: opencode for dports-* agents)
    OPENCODE_MODEL            (optional) e.g., gpt-5-nano (default: gpt-5-nano for dports-* agents)
    OPENCODE_TIMEOUT          (optional) request timeout in seconds, default 120
    OPENCODE_MAX_RETRIES      (optional) retry attempts, default 3
    OPENCODE_RETRY_DELAY      (optional) base delay between retries, default 8
    OPENCODE_MAX_SNIPPET_ROUNDS (optional) max snippet extraction rounds, default 5

    VM_SSH_HOST               (optional) SSH host for remote snippet extraction
                              If set, snippet-extractor runs via SSH (for Linux dev hosts)
                              If not set, snippet-extractor runs locally (DragonFlyBSD native)
    VM_SSH_KEY                (optional) SSH key for VM access (only used if VM_SSH_HOST set)
    VM_SSH_PORT               (optional) SSH port (only used if VM_SSH_HOST set), default 2222

Job types:
    type=triage (default) - Uses dports-triage agent, may auto-enqueue patch job
    type=patch            - Uses dports-patch agent to generate a diff
    type=apply            - Applies patch, rebuilds, creates PR if successful

Job fields:
    snippet_round=N       - Current snippet round (0 = initial, no snippets)
    has_snippets=true     - Snippets are available in bundle
    parent_job=...        - Parent job filename (for tracking lineage)
    iteration=N           - Current fix iteration (1-based, for apply jobs)
    max_iterations=N      - Maximum iterations before giving up (default: 3)
    previous_bundle=...   - Bundle from previous failed attempt (if iteration > 1)
"""

import argparse
import json
import os
import re
import subprocess
import sys
import time
import urllib.request
import urllib.error
from datetime import datetime, timezone
from pathlib import Path


# Classifications that should trigger automatic patch generation
PATCHABLE_CLASSIFICATIONS = {
    "compile-error",
    "configure-error",
    "patch-error",
    "plist-error",
}

# Confidence levels that allow patch generation
PATCHABLE_CONFIDENCE = {"high", "medium"}

# Max snippet escalation rounds (default, can override with env var)
DEFAULT_MAX_SNIPPET_ROUNDS = 5

# Max fix iterations before giving up on a port
DEFAULT_MAX_ITERATIONS = 3

# VM SSH defaults
DEFAULT_VM_SSH_KEY = "/home/antonioh/.go-synth/vm/id_ed25519"
DEFAULT_VM_SSH_PORT = "2222"
DEFAULT_VM_SSH_HOST = "root@localhost"


def log(queue_root: Path, level: str, message: str):
    """Log to both stderr and runner.log."""
    ts = datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")
    line = f"{ts} {level:5} {message}"
    print(line, file=sys.stderr)
    try:
        with open(queue_root / "runner.log", "a") as f:
            f.write(line + "\n")
    except OSError:
        pass


def parse_job_file(path: Path) -> dict:
    """Parse key=value job file into dict."""
    data = {}
    with open(path) as f:
        for line in f:
            line = line.strip()
            if "=" in line:
                key, _, value = line.partition("=")
                data[key] = value
    return data


def read_file_if_exists(path: Path, max_bytes: int = 200_000) -> str | None:
    """Read file contents if it exists, truncate if too large."""
    if not path.exists():
        return None
    try:
        content = path.read_text(errors="replace")
        if len(content) > max_bytes:
            content = content[:max_bytes] + "\n[...truncated...]\n"
        return content
    except OSError:
        return None


def find_kedb_dir() -> Path | None:
    """Find the KEDB directory relative to this script or in DeltaPorts repo."""
    script_dir = Path(__file__).resolve().parent
    kedb_dir = script_dir.parent / "docs" / "kedb"
    if kedb_dir.exists():
        return kedb_dir
    return None


def load_kedb(kedb_dir: Path | None) -> str:
    """Load all KEDB markdown files into a single context block."""
    if not kedb_dir or not kedb_dir.exists():
        return ""
    
    kedb_files = sorted(kedb_dir.glob("*.md"))
    skip_files = {"readme.md", "template.md"}
    kedb_files = [f for f in kedb_files if f.name.lower() not in skip_files]
    
    if not kedb_files:
        return ""
    
    parts = ["## Known Error Database (KEDB)", ""]
    parts.append("The following are known DragonFlyBSD-specific build issues and their fixes:")
    parts.append("")
    
    for kf in kedb_files:
        content = read_file_if_exists(kf, max_bytes=50_000)
        if content:
            parts.append(f"### {kf.stem}")
            parts.append(content)
            parts.append("")
    
    return "\n".join(parts)


# -----------------------------------------------------------------------------
# Triage parsing
# -----------------------------------------------------------------------------

def parse_triage_output(triage_path: Path) -> dict:
    """Extract Classification and Confidence from triage.md."""
    result = {"classification": "", "confidence": "", "raw": ""}
    
    content = read_file_if_exists(triage_path)
    if not content:
        return result
    
    result["raw"] = content
    
    # Extract Classification
    match = re.search(r"^##\s*Classification\s*\n+([^\n#]+)", content, re.MULTILINE | re.IGNORECASE)
    if match:
        result["classification"] = match.group(1).strip().lower()
    
    # Extract Confidence
    match = re.search(r"^##\s*Confidence\s*\n+([^\n#]+)", content, re.MULTILINE | re.IGNORECASE)
    if match:
        result["confidence"] = match.group(1).strip().lower()
    
    return result


def should_enqueue_patch(triage: dict) -> bool:
    """Determine if a patch job should be auto-enqueued based on triage results."""
    classification = triage.get("classification", "")
    confidence = triage.get("confidence", "")
    
    if classification not in PATCHABLE_CLASSIFICATIONS:
        return False
    if confidence not in PATCHABLE_CONFIDENCE:
        return False
    return True


# -----------------------------------------------------------------------------
# Snippet escalation
# -----------------------------------------------------------------------------

def parse_snippet_requests(content: str) -> list[str]:
    """Parse ## Snippet Requests section from markdown, return raw request lines."""
    match = re.search(
        r"^##\s*Snippet Requests\s*\n(.*?)(?=^##|\Z)",
        content,
        re.MULTILINE | re.DOTALL | re.IGNORECASE
    )
    if not match:
        return []
    
    section = match.group(1)
    requests = []
    
    for line in section.split("\n"):
        line = line.strip()
        if not line:
            continue
        # Parse: - `request:spec` â€” description
        m = re.match(r"-\s*`([^`]+)`", line)
        if m:
            requests.append(m.group(1))
    
    return requests


def get_vm_ssh_command() -> list[str]:
    """Build base SSH command for VM access."""
    ssh_key = os.environ.get("VM_SSH_KEY", DEFAULT_VM_SSH_KEY)
    ssh_port = os.environ.get("VM_SSH_PORT", DEFAULT_VM_SSH_PORT)
    ssh_host = os.environ.get("VM_SSH_HOST", DEFAULT_VM_SSH_HOST)
    
    return [
        "ssh",
        "-i", ssh_key,
        "-p", ssh_port,
        "-o", "StrictHostKeyChecking=no",
        "-o", "UserKnownHostsFile=/dev/null",
        "-o", "BatchMode=yes",
        "-o", "ConnectTimeout=10",
        ssh_host,
    ]


def run_snippet_extractor(
    queue_root: Path,
    bundle_dir: Path,
    round_num: int,
    prefer_workdir: bool = True
) -> tuple[bool, str]:
    """
    Run snippet-extractor, either locally or via SSH to VM.
    
    Mode selection:
    - If VM_SSH_HOST is set: SSH to VM and run remotely
    - Otherwise: Run snippet-extractor directly on local system
    
    Returns (success, error_message).
    """
    # Build extractor arguments
    extractor_args = [
        "--bundle", str(bundle_dir),
        "--round", str(round_num),
    ]
    if prefer_workdir:
        extractor_args.append("--prefer-workdir")
    
    # Determine execution mode based on VM_SSH_HOST presence
    vm_ssh_host = os.environ.get("VM_SSH_HOST")
    
    if vm_ssh_host:
        # SSH mode: run on remote VM
        ssh_cmd = get_vm_ssh_command()
        full_cmd = ssh_cmd + ["/usr/local/bin/snippet-extractor"] + extractor_args
        mode_desc = f"on VM ({vm_ssh_host})"
    else:
        # Local mode: run directly
        # Try to find snippet-extractor in common locations
        extractor_path = None
        candidates = [
            "/usr/local/bin/snippet-extractor",
            str(Path(__file__).parent / "snippet-extractor"),
        ]
        for candidate in candidates:
            if Path(candidate).exists():
                extractor_path = candidate
                break
        
        if not extractor_path:
            return False, "snippet-extractor not found in expected locations"
        
        full_cmd = [extractor_path] + extractor_args
        mode_desc = "locally"
    
    log(queue_root, "INFO", f"running snippet-extractor round {round_num} {mode_desc}")
    
    try:
        result = subprocess.run(
            full_cmd,
            capture_output=True,
            text=True,
            timeout=120
        )
        
        # Log output
        if result.stdout:
            for line in result.stdout.strip().split("\n"):
                log(queue_root, "DEBUG", f"extractor: {line}")
        if result.stderr:
            for line in result.stderr.strip().split("\n"):
                log(queue_root, "DEBUG", f"extractor: {line}")
        
        if result.returncode == 0:
            log(queue_root, "INFO", f"snippet extraction round {round_num} succeeded")
            return True, ""
        elif result.returncode == 1:
            return False, "No snippet requests found"
        elif result.returncode == 2:
            return False, "All snippet requests failed"
        else:
            return False, f"Extractor exit code {result.returncode}"
            
    except subprocess.TimeoutExpired:
        return False, "Snippet extraction timed out"
    except Exception as e:
        error_type = "SSH/extractor" if vm_ssh_host else "Extractor"
        return False, f"{error_type} error: {e}"


def build_snippet_feedback(bundle_dir: Path, round_num: int) -> str:
    """Generate feedback section from snippet manifest for agent context."""
    manifest_path = bundle_dir / "analysis" / "snippets" / "manifest.json"
    if not manifest_path.exists():
        return ""
    
    try:
        with open(manifest_path) as f:
            manifest = json.load(f)
    except Exception:
        return ""
    
    rounds = manifest.get("rounds", [])
    if not rounds:
        return ""
    
    # Find the latest round
    latest_round = None
    for r in rounds:
        if r.get("round") == round_num:
            latest_round = r
            break
    
    if not latest_round:
        # Use the last round
        latest_round = rounds[-1]
    
    parts = ["## Snippet Extraction Results", ""]
    parts.append(f"**Round {latest_round.get('round', '?')}** | Source: `{latest_round.get('source', 'unknown')}` | Budget remaining: {latest_round.get('budget_remaining', 0)} bytes")
    parts.append("")
    
    requests = latest_round.get("requests", [])
    if requests:
        parts.append("| Request | Status | Output | Bytes |")
        parts.append("|---------|--------|--------|-------|")
        for req in requests:
            raw = req.get("raw", "?")[:40]
            status = req.get("status", "?")
            output = req.get("output", "-")
            if output and len(output) > 30:
                output = "..." + output[-27:]
            bytes_ = req.get("bytes", 0)
            note = req.get("note", "")
            
            # Add emoji for status
            status_display = {
                "ok": "ok",
                "not_found": "not_found",
                "budget_exceeded": "budget_exceeded",
                "empty": "empty",
            }.get(status, status)
            
            parts.append(f"| `{raw}` | {status_display} | {output or '-'} | {bytes_} |")
            if note:
                parts.append(f"|  | *{note}* | | |")
    
    parts.append("")
    
    # Add summary
    total_rounds = manifest.get("total_rounds", 0)
    max_rounds = int(os.environ.get("OPENCODE_MAX_SNIPPET_ROUNDS", DEFAULT_MAX_SNIPPET_ROUNDS))
    remaining_rounds = max_rounds - total_rounds
    
    parts.append(f"**Snippet rounds used:** {total_rounds}/{max_rounds} (remaining: {remaining_rounds})")
    if remaining_rounds <= 0:
        parts.append("**NOTE:** No more snippet rounds available. Work with the information provided.")
    parts.append("")
    
    return "\n".join(parts)


def load_snippets_content(bundle_dir: Path, round_num: int, max_bytes: int = 200_000) -> str:
    """Load extracted snippet contents for inclusion in payload."""
    round_dir = bundle_dir / "analysis" / "snippets" / f"round_{round_num}"
    if not round_dir.exists():
        return ""
    
    parts = ["## Extracted Snippets", ""]
    total_bytes = 0
    
    # Load round manifest for context
    manifest_path = round_dir / "manifest.json"
    if manifest_path.exists():
        try:
            with open(manifest_path) as f:
                round_manifest = json.load(f)
            source_type = round_manifest.get("source", "unknown")
            distfile = round_manifest.get("distfile")
            if distfile:
                parts.append(f"*Source: distfile `{distfile}`*")
            else:
                parts.append(f"*Source: {source_type}*")
            parts.append("")
        except Exception:
            pass
    
    # Walk through subdirectories (source, buildsystem, configure, log)
    for subdir in sorted(round_dir.iterdir()):
        if not subdir.is_dir() or subdir.name.startswith("."):
            continue
        
        for file_path in sorted(subdir.glob("*.txt")):
            if total_bytes >= max_bytes:
                parts.append(f"*[...truncated, budget exceeded...]*")
                break
            
            try:
                content = file_path.read_text(errors="replace")
                remaining = max_bytes - total_bytes
                if len(content) > remaining:
                    content = content[:remaining] + "\n[...truncated...]\n"
                
                # Infer original filename from safe name
                original_name = file_path.stem.replace("_", "/")
                
                parts.append(f"### {subdir.name}/{original_name}")
                parts.append("```")
                parts.append(content)
                parts.append("```")
                parts.append("")
                
                total_bytes += len(content)
            except Exception:
                continue
        
        if total_bytes >= max_bytes:
            break
    
    if total_bytes == 0:
        return ""
    
    return "\n".join(parts)


def enqueue_followup_job(
    queue_root: Path,
    job: dict,
    job_type: str,
    new_round: int,
    parent_job_name: str
) -> Path:
    """Create a follow-up job with incremented snippet round."""
    ts = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%SZ")
    origin_safe = job.get("origin", "unknown").replace("/", "_")
    pid = os.getpid()
    
    job_name = f"{ts}-{job.get('profile', 'unknown')}-{origin_safe}-{pid}-{job_type}-r{new_round}.job"
    
    pending_dir = queue_root / "pending"
    job_path = pending_dir / job_name
    
    content = [
        f"type={job_type}",
        f"created_ts_utc={ts}",
        f"profile={job.get('profile', '')}",
        f"origin={job.get('origin', '')}",
        f"flavor={job.get('flavor', '')}",
        f"bundle_dir={job.get('bundle_dir', '')}",
        f"run_id={job.get('run_id', '')}",
        f"snippet_round={new_round}",
        f"has_snippets=true",
        f"parent_job={parent_job_name}",
    ]
    
    # Preserve triage_file for patch jobs
    if job_type == "patch":
        triage_file = job.get("triage_file", f"{job.get('bundle_dir', '')}/analysis/triage.md")
        content.append(f"triage_file={triage_file}")
    
    # Atomic write
    tmp_path = job_path.with_suffix(".tmp")
    with open(tmp_path, "w") as f:
        f.write("\n".join(content) + "\n")
    tmp_path.rename(job_path)
    
    return job_path


def check_and_handle_snippet_requests(
    queue_root: Path,
    job: dict,
    job_type: str,
    job_path: Path,
    bundle_dir: Path,
    response_text: str
) -> tuple[bool, str]:
    """
    Check if agent requested snippets and handle extraction.
    Returns (should_continue_processing, message).
    
    If snippets were requested and we haven't hit max rounds:
    - Run extractor on VM
    - Enqueue follow-up job
    - Return (False, "enqueued follow-up")
    
    Otherwise return (True, "") to continue normal processing.
    """
    # Parse snippet requests from response
    requests = parse_snippet_requests(response_text)
    if not requests:
        return True, ""
    
    current_round = int(job.get("snippet_round", "0"))
    max_rounds = int(os.environ.get("OPENCODE_MAX_SNIPPET_ROUNDS", DEFAULT_MAX_SNIPPET_ROUNDS))
    
    if current_round >= max_rounds:
        log(queue_root, "WARN", f"snippet requests found but max rounds ({max_rounds}) reached")
        return True, ""
    
    new_round = current_round + 1
    log(queue_root, "INFO", f"found {len(requests)} snippet requests, starting round {new_round}")
    
    # Run extractor on VM
    success, error = run_snippet_extractor(queue_root, bundle_dir, new_round)
    if not success:
        log(queue_root, "WARN", f"snippet extraction failed: {error}")
        # Continue processing anyway - agent will work without snippets
        return True, ""
    
    # Enqueue follow-up job
    followup_path = enqueue_followup_job(
        queue_root, job, job_type, new_round, job_path.name
    )
    log(queue_root, "INFO", f"enqueued follow-up job: {followup_path.name}")
    
    return False, f"enqueued snippet round {new_round}"


# -----------------------------------------------------------------------------
# Diff validation
# -----------------------------------------------------------------------------

def validate_diff(diff_text: str) -> tuple[bool, str]:
    """
    Validate unified diff syntax.
    Returns (is_valid, error_message).
    """
    if not diff_text or not diff_text.strip():
        return False, "Empty diff"
    
    lines = diff_text.strip().split("\n")
    
    # Must have --- and +++ headers
    has_old_file = any(line.startswith("---") for line in lines)
    has_new_file = any(line.startswith("+++") for line in lines)
    
    if not has_old_file:
        return False, "Missing '---' file header"
    if not has_new_file:
        return False, "Missing '+++' file header"
    
    # Must have at least one hunk header
    has_hunk = any(line.startswith("@@") for line in lines)
    if not has_hunk:
        return False, "Missing '@@ ... @@' hunk header"
    
    # Check that lines in hunks have valid prefixes
    in_hunk = False
    for line in lines:
        if line.startswith("@@"):
            in_hunk = True
            continue
        if in_hunk:
            # End of hunk detection (next file or end)
            if line.startswith("---") or line.startswith("+++") or line.startswith("diff "):
                in_hunk = False
                continue
            # Valid hunk line prefixes: +, -, space, or empty (for context)
            if line and not line[0] in ("+", "-", " ", "\\"):
                # Allow "\ No newline at end of file"
                if not line.startswith("\\"):
                    return False, f"Invalid line prefix in hunk: {line[:50]}"
    
    return True, ""


def extract_diff_from_response(text: str) -> str | None:
    """Extract the diff block from the agent's response."""
    # Look for ```diff ... ``` block
    match = re.search(r"```diff\s*\n(.*?)```", text, re.DOTALL)
    if match:
        return match.group(1).strip()
    
    # Fallback: look for content starting with "---" or "diff --git"
    lines = text.split("\n")
    diff_start = None
    for i, line in enumerate(lines):
        if line.startswith("---") or line.startswith("diff --git") or line.startswith("diff -"):
            diff_start = i
            break
    
    if diff_start is not None:
        # Take everything from diff start until we hit a section marker
        # Diff ends when we see text that's clearly not part of a diff
        diff_lines = []
        in_hunk = False
        for line in lines[diff_start:]:
            # Stop on markdown headers
            if line.startswith("## ") and not line.startswith("## Patch"):
                break
            # Stop on common section markers (without ## prefix)
            if line.strip() in ("Rationale", "Files Modified", "Testing Notes", "Snippet Requests"):
                break
            # Stop if line looks like prose after a hunk (starts with capital, no diff prefix)
            if in_hunk and line and not line[0] in " +-@\\":
                # Check if it looks like a new file header or prose
                if not line.startswith("---") and not line.startswith("+++") and not line.startswith("diff"):
                    # Likely end of diff - prose text
                    if len(line) > 10 and line[0].isupper() and " " in line[:20]:
                        break
            if line.startswith("@@"):
                in_hunk = True
            diff_lines.append(line)
        return "\n".join(diff_lines).strip()
    
    return None


# -----------------------------------------------------------------------------
# Payload builders
# -----------------------------------------------------------------------------

def build_triage_payload(
    bundle_dir: Path,
    kedb_dir: Path | None = None,
    job: dict | None = None
) -> str:
    """Build the triage prompt from bundle contents."""
    parts = []
    
    # Include snippet feedback and content if this is a follow-up round
    snippet_round = int(job.get("snippet_round", "0")) if job else 0
    has_snippets = job.get("has_snippets", "false") == "true" if job else False
    
    if has_snippets and snippet_round > 0:
        # Add feedback about previous extraction
        feedback = build_snippet_feedback(bundle_dir, snippet_round)
        if feedback:
            parts.append(feedback)
            parts.append("")
        
        # Add extracted snippet contents
        snippet_content = load_snippets_content(bundle_dir, snippet_round)
        if snippet_content:
            parts.append(snippet_content)
            parts.append("")
    
    # Known Error Database (if available)
    kedb_content = load_kedb(kedb_dir)
    if kedb_content:
        parts.append(kedb_content)
        parts.append("")
    
    # Metadata
    meta = read_file_if_exists(bundle_dir / "meta.txt")
    if meta:
        parts.append("## Metadata")
        parts.append(meta)
        parts.append("")
    
    # Build errors
    errors = read_file_if_exists(bundle_dir / "logs" / "errors.txt")
    if errors:
        parts.append("## Build Errors")
        parts.append(errors)
        parts.append("")
    
    # Port files
    parts.append("## Port Files")
    
    makefile = read_file_if_exists(bundle_dir / "port" / "Makefile")
    if makefile:
        parts.append("### Makefile")
        parts.append("```makefile")
        parts.append(makefile)
        parts.append("```")
        parts.append("")
    
    plist = read_file_if_exists(bundle_dir / "port" / "pkg-plist")
    if plist:
        parts.append("### pkg-plist")
        parts.append("```")
        parts.append(plist)
        parts.append("```")
        parts.append("")
    
    distinfo = read_file_if_exists(bundle_dir / "port" / "distinfo")
    if distinfo:
        parts.append("### distinfo")
        parts.append("```")
        parts.append(distinfo)
        parts.append("```")
        parts.append("")
    
    # Patches
    patches_dir = bundle_dir / "port" / "files"
    if patches_dir.exists():
        patch_files = sorted(patches_dir.glob("patch-*"))
        if patch_files:
            parts.append("### Existing Patches")
            for pf in patch_files:
                content = read_file_if_exists(pf)
                if content:
                    parts.append(f"#### {pf.name}")
                    parts.append("```diff")
                    parts.append(content)
                    parts.append("```")
                    parts.append("")
    
    parts.append("---")
    parts.append("Analyze this build failure and provide your triage report.")
    
    return "\n".join(parts)


def build_patch_payload(
    bundle_dir: Path,
    kedb_dir: Path | None = None,
    job: dict | None = None
) -> str:
    """Build the patch generation prompt including triage output."""
    parts = []
    
    # Include snippet feedback and content if this is a follow-up round
    snippet_round = int(job.get("snippet_round", "0")) if job else 0
    has_snippets = job.get("has_snippets", "false") == "true" if job else False
    
    if has_snippets and snippet_round > 0:
        # Add feedback about previous extraction
        feedback = build_snippet_feedback(bundle_dir, snippet_round)
        if feedback:
            parts.append(feedback)
            parts.append("")
        
        # Add extracted snippet contents
        snippet_content = load_snippets_content(bundle_dir, snippet_round)
        if snippet_content:
            parts.append(snippet_content)
            parts.append("")
    
    # Triage summary (most important context)
    triage = read_file_if_exists(bundle_dir / "analysis" / "triage.md")
    if triage:
        parts.append("## Triage Summary")
        parts.append(triage)
        parts.append("")
    
    # Known Error Database (if available)
    kedb_content = load_kedb(kedb_dir)
    if kedb_content:
        parts.append(kedb_content)
        parts.append("")
    
    # Metadata
    meta = read_file_if_exists(bundle_dir / "meta.txt")
    if meta:
        parts.append("## Metadata")
        parts.append(meta)
        parts.append("")
    
    # Build errors
    errors = read_file_if_exists(bundle_dir / "logs" / "errors.txt")
    if errors:
        parts.append("## Build Errors")
        parts.append(errors)
        parts.append("")
    
    # Port files
    parts.append("## Port Files")
    
    makefile = read_file_if_exists(bundle_dir / "port" / "Makefile")
    if makefile:
        parts.append("### Makefile")
        parts.append("```makefile")
        parts.append(makefile)
        parts.append("```")
        parts.append("")
    
    plist = read_file_if_exists(bundle_dir / "port" / "pkg-plist")
    if plist:
        parts.append("### pkg-plist")
        parts.append("```")
        parts.append(plist)
        parts.append("```")
        parts.append("")
    
    distinfo = read_file_if_exists(bundle_dir / "port" / "distinfo")
    if distinfo:
        parts.append("### distinfo")
        parts.append("```")
        parts.append(distinfo)
        parts.append("```")
        parts.append("")
    
    # Existing patches
    patches_dir = bundle_dir / "port" / "files"
    if patches_dir.exists():
        patch_files = sorted(patches_dir.glob("patch-*"))
        if patch_files:
            parts.append("### Existing Patches")
            for pf in patch_files:
                content = read_file_if_exists(pf)
                if content:
                    parts.append(f"#### {pf.name}")
                    parts.append("```diff")
                    parts.append(content)
                    parts.append("```")
                    parts.append("")
    
    # Check if Makefile.DragonFly exists in bundle
    makefile_dfly = read_file_if_exists(bundle_dir / "port" / "Makefile.DragonFly")
    has_makefile_dfly = makefile_dfly is not None
    
    if has_makefile_dfly:
        parts.append("### Makefile.DragonFly (existing - modify this file)")
        parts.append("```makefile")
        parts.append(makefile_dfly)
        parts.append("```")
        parts.append("")
    else:
        parts.append("### Makefile.DragonFly")
        parts.append("**This file does not exist yet. You must CREATE it.**")
        parts.append("")
    
    parts.append("---")
    
    # Build instructions with context about whether file exists
    if has_makefile_dfly:
        file_instruction = """
IMPORTANT: Makefile.DragonFly already exists. Your diff should MODIFY the existing file.
Use `--- a/ports/...` and `+++ b/ports/...` format with proper context lines."""
    else:
        file_instruction = """
IMPORTANT: Makefile.DragonFly does NOT exist yet. Your diff must CREATE a new file.
You MUST use `--- /dev/null` (not `--- a/...`) as the source."""
    
    parts.append(f"""
## Instructions

Generate a patch that fixes this issue for the DeltaPorts overlay.
{file_instruction}

### Output Format

Output a single unified diff in a ```diff code block. The diff must:

1. Use paths relative to DeltaPorts overlay root: `ports/category/portname/...`
2. Target `Makefile.DragonFly` for DragonFly-specific fixes (preferred)
3. Use `diffs/*.diff` only when patching upstream source files

### Creating New Files (when Makefile.DragonFly does NOT exist)

```diff
--- /dev/null
+++ b/ports/net/example/Makefile.DragonFly
@@ -0,0 +1,1 @@
+OPTIONS_DEFAULT:=	${{OPTIONS_DEFAULT:NFEATURENAME}}
```

CRITICAL for new files:
- First line MUST be `--- /dev/null` (literally /dev/null, not a path)
- Hunk header MUST be `@@ -0,0 +1,N @@` where N = number of lines
- ALL content lines MUST start with `+`
- Do NOT include `-` lines (there's nothing to remove from /dev/null)

### Modifying Existing Files (when Makefile.DragonFly exists)

```diff
--- a/ports/net/example/Makefile.DragonFly
+++ b/ports/net/example/Makefile.DragonFly
@@ -1,2 +1,3 @@
 # Existing line
+NEW_LINE=value
 Another existing line
```

### Common Fix Patterns

- Disable FreeBSD-only option: `OPTIONS_DEFAULT:= ${{OPTIONS_DEFAULT:NOPTIONNAME}}`
- Add plist entry: `PLIST_FILES+= path/to/file`
- Multiple option removals: `OPTIONS_DEFAULT:= ${{OPTIONS_DEFAULT:NOPT1:NOPT2}}`
""".strip())
    
    return "\n".join(parts)


# -----------------------------------------------------------------------------
# API calls
# -----------------------------------------------------------------------------

def call_opencode(
    url: str,
    payload: str,
    agent: str | None,
    provider: str | None,
    model: str | None,
    timeout: int
) -> dict:
    """
    Call opencode serve API:
    1. POST /session to create session
    2. POST /session/<id>/message to send payload
    Returns the full response dict.
    """
    headers = {"Content-Type": "application/json"}
    
    # Create session
    session_url = f"{url.rstrip('/')}/session"
    req = urllib.request.Request(
        session_url,
        data=json.dumps({}).encode(),
        headers=headers,
        method="POST"
    )
    with urllib.request.urlopen(req, timeout=timeout) as resp:
        session_data = json.loads(resp.read().decode())
    
    session_id = session_data.get("id")
    if not session_id:
        raise RuntimeError(f"No session ID in response: {session_data}")
    
    # Send message
    message_url = f"{url.rstrip('/')}/session/{session_id}/message"
    message_body = {
        "parts": [{"type": "text", "text": payload}]
    }
    
    # Agent must be specified in the message body for subagents
    if agent:
        message_body["agent"] = agent
    
    # Add model specification if provided
    if provider and model:
        message_body["model"] = {"providerID": provider, "modelID": model}
    elif model:
        message_body["model"] = {"providerID": "opencode", "modelID": model}
    
    req = urllib.request.Request(
        message_url,
        data=json.dumps(message_body).encode(),
        headers=headers,
        method="POST"
    )
    with urllib.request.urlopen(req, timeout=timeout) as resp:
        response_data = json.loads(resp.read().decode())
    
    response_data["_session_id"] = session_id
    return response_data


def extract_response_text(response: dict) -> str:
    """Extract the assistant's text response from API response."""
    info = response.get("info", {})
    error = info.get("error")
    if error:
        error_name = error.get("name", "UnknownError")
        error_data = error.get("data", {})
        error_msg = error_data.get("message", str(error_data))
        return f"# Error: {error_name}\n\n{error_msg}"
    
    parts = response.get("parts", [])
    text_parts = []
    for part in parts:
        if isinstance(part, dict) and part.get("type") == "text":
            text_parts.append(part.get("text", ""))
        elif isinstance(part, str):
            text_parts.append(part)
    
    if text_parts:
        return "\n".join(text_parts)
    
    return json.dumps(response, indent=2)


# -----------------------------------------------------------------------------
# Output writers
# -----------------------------------------------------------------------------

def write_triage_outputs(bundle_dir: Path, response: dict, session_id: str):
    """Write triage analysis outputs to bundle."""
    analysis_dir = bundle_dir / "analysis"
    analysis_dir.mkdir(exist_ok=True)
    
    with open(analysis_dir / "triage.json", "w") as f:
        json.dump(response, f, indent=2)
    
    with open(analysis_dir / "session_id.txt", "w") as f:
        f.write(session_id + "\n")
    
    text = extract_response_text(response)
    with open(analysis_dir / "triage.md", "w") as f:
        f.write(text + "\n")


def write_patch_outputs(bundle_dir: Path, response: dict, session_id: str) -> tuple[bool, str]:
    """
    Write patch outputs to bundle.
    Returns (success, error_message).
    """
    analysis_dir = bundle_dir / "analysis"
    analysis_dir.mkdir(exist_ok=True)
    
    # Write raw response
    with open(analysis_dir / "patch.json", "w") as f:
        json.dump(response, f, indent=2)
    
    # Write session ID
    with open(analysis_dir / "patch_session_id.txt", "w") as f:
        f.write(session_id + "\n")
    
    # Extract and validate diff
    text = extract_response_text(response)
    with open(analysis_dir / "patch.md", "w") as f:
        f.write(text + "\n")
    
    diff = extract_diff_from_response(text)
    if not diff:
        return False, "No diff block found in response"
    
    is_valid, error = validate_diff(diff)
    if not is_valid:
        # Write invalid diff with error note
        with open(analysis_dir / "patch.diff.invalid", "w") as f:
            f.write(f"# VALIDATION ERROR: {error}\n\n")
            f.write(diff)
        return False, f"Invalid diff: {error}"
    
    # Write valid diff
    with open(analysis_dir / "patch.diff", "w") as f:
        f.write(diff + "\n")
    
    return True, ""


# -----------------------------------------------------------------------------
# Job management
# -----------------------------------------------------------------------------

def move_job(job_path: Path, dest_dir: str) -> Path:
    """Move job file to destination directory (done/failed)."""
    dest = job_path.parent.parent / dest_dir / job_path.name
    job_path.rename(dest)
    return dest


def write_error_note(job_path: Path, error: str):
    """Write error note next to failed job."""
    error_path = job_path.with_suffix(".job.error")
    with open(error_path, "w") as f:
        f.write(f"timestamp={datetime.now(timezone.utc).isoformat()}\n")
        f.write(f"error={error}\n")


def claim_next_job(queue_root: Path) -> Path | None:
    """Claim the oldest pending job by moving to inflight/."""
    pending_dir = queue_root / "pending"
    inflight_dir = queue_root / "inflight"
    
    jobs = sorted(pending_dir.glob("*.job"))
    
    for job_path in jobs:
        try:
            dest = inflight_dir / job_path.name
            job_path.rename(dest)
            return dest
        except OSError:
            continue
    
    return None


def enqueue_patch_job(queue_root: Path, job: dict):
    """Enqueue a patch job based on completed triage job."""
    ts = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%SZ")
    origin_safe = job.get("origin", "unknown").replace("/", "_")
    pid = os.getpid()
    
    job_name = f"{ts}-{job.get('profile', 'unknown')}-{origin_safe}-{pid}-patch.job"
    
    pending_dir = queue_root / "pending"
    job_path = pending_dir / job_name
    
    content = [
        f"type=patch",
        f"created_ts_utc={ts}",
        f"profile={job.get('profile', '')}",
        f"origin={job.get('origin', '')}",
        f"flavor={job.get('flavor', '')}",
        f"bundle_dir={job.get('bundle_dir', '')}",
        f"run_id={job.get('run_id', '')}",
        f"triage_file={job.get('bundle_dir', '')}/analysis/triage.md",
    ]
    
    # Atomic write
    tmp_path = job_path.with_suffix(".tmp")
    with open(tmp_path, "w") as f:
        f.write("\n".join(content) + "\n")
    tmp_path.rename(job_path)
    
    return job_path


def enqueue_apply_job(queue_root: Path, job: dict, parent_job_name: str) -> Path:
    """Enqueue an apply job based on completed patch job."""
    ts = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%SZ")
    origin_safe = job.get("origin", "unknown").replace("/", "_")
    pid = os.getpid()
    
    job_name = f"{ts}-{job.get('profile', 'unknown')}-{origin_safe}-{pid}-apply.job"
    
    pending_dir = queue_root / "pending"
    job_path = pending_dir / job_name
    
    # Inherit iteration from parent job, or start at 1
    iteration = int(job.get("iteration", "1"))
    max_iterations = int(job.get("max_iterations", str(DEFAULT_MAX_ITERATIONS)))
    
    content = [
        f"type=apply",
        f"created_ts_utc={ts}",
        f"profile={job.get('profile', '')}",
        f"origin={job.get('origin', '')}",
        f"flavor={job.get('flavor', '')}",
        f"bundle_dir={job.get('bundle_dir', '')}",
        f"run_id={job.get('run_id', '')}",
        f"iteration={iteration}",
        f"max_iterations={max_iterations}",
        f"parent_job={parent_job_name}",
    ]
    
    # Include previous_bundle if this is a retry
    previous_bundle = job.get("previous_bundle")
    if previous_bundle:
        content.append(f"previous_bundle={previous_bundle}")
    
    # Atomic write
    tmp_path = job_path.with_suffix(".tmp")
    with open(tmp_path, "w") as f:
        f.write("\n".join(content) + "\n")
    tmp_path.rename(job_path)
    
    return job_path


# -----------------------------------------------------------------------------
# Job processing
# -----------------------------------------------------------------------------

def process_triage_job(
    queue_root: Path,
    job_path: Path,
    job: dict,
    bundle_dir: Path,
    opencode_url: str,
    opencode_provider: str | None,
    opencode_model: str | None,
    timeout: int,
    max_retries: int,
    retry_delay: int,
    kedb_dir: Path | None
) -> tuple[bool, str]:
    """
    Process a triage job.
    Returns (success, status_message).
    Status message can be "done", "snippet_followup", or error text.
    """
    agent = "dports-triage"
    payload = build_triage_payload(bundle_dir, kedb_dir, job)
    snippet_round = int(job.get("snippet_round", "0"))
    
    last_error = None
    model_info = f"{opencode_provider}/{opencode_model}" if opencode_provider and opencode_model else "agent-default"
    
    for attempt in range(max_retries):
        try:
            log(queue_root, "INFO", f"calling opencode (attempt {attempt + 1}/{max_retries}, agent={agent}, model={model_info}, round={snippet_round})")
            response = call_opencode(opencode_url, payload, agent, opencode_provider, opencode_model, timeout)
            session_id = response.get("_session_id", "unknown")
            
            write_triage_outputs(bundle_dir, response, session_id)
            log(queue_root, "INFO", f"wrote triage to {bundle_dir}/analysis/")
            
            # Check for snippet requests before deciding on patch job
            response_text = extract_response_text(response)
            should_continue, snippet_msg = check_and_handle_snippet_requests(
                queue_root, job, "triage", job_path, bundle_dir, response_text
            )
            
            if not should_continue:
                # Snippet follow-up was enqueued
                return True, "snippet_followup"
            
            # Check if we should auto-enqueue a patch job
            triage = parse_triage_output(bundle_dir / "analysis" / "triage.md")
            if should_enqueue_patch(triage):
                patch_job_path = enqueue_patch_job(queue_root, job)
                log(queue_root, "INFO", f"auto-enqueued patch job: {patch_job_path.name} (classification={triage['classification']}, confidence={triage['confidence']})")
            else:
                log(queue_root, "INFO", f"no patch job enqueued (classification={triage['classification']}, confidence={triage['confidence']})")
            
            return True, "done"
            
        except (urllib.error.URLError, urllib.error.HTTPError, OSError, RuntimeError) as e:
            last_error = str(e)
            log(queue_root, "WARN", f"attempt {attempt + 1} failed: {last_error}")
            
            if attempt < max_retries - 1:
                delay = min(retry_delay * (2 ** attempt), 60)
                log(queue_root, "INFO", f"retrying in {delay}s...")
                time.sleep(delay)
    
    write_error_note(job_path, last_error or "unknown error")
    return False, last_error or "unknown error"


def process_patch_job(
    queue_root: Path,
    job_path: Path,
    job: dict,
    bundle_dir: Path,
    opencode_url: str,
    opencode_provider: str | None,
    opencode_model: str | None,
    timeout: int,
    max_retries: int,
    retry_delay: int,
    kedb_dir: Path | None
) -> tuple[bool, str]:
    """
    Process a patch job.
    Returns (success, status_message).
    Status message can be "done", "snippet_followup", or error text.
    """
    agent = "dports-patch"
    payload = build_patch_payload(bundle_dir, kedb_dir, job)
    snippet_round = int(job.get("snippet_round", "0"))
    
    last_error = None
    model_info = f"{opencode_provider}/{opencode_model}" if opencode_provider and opencode_model else "agent-default"
    
    for attempt in range(max_retries):
        try:
            log(queue_root, "INFO", f"calling opencode (attempt {attempt + 1}/{max_retries}, agent={agent}, model={model_info}, round={snippet_round})")
            response = call_opencode(opencode_url, payload, agent, opencode_provider, opencode_model, timeout)
            session_id = response.get("_session_id", "unknown")
            
            # Write outputs first (before checking for snippets)
            success, error = write_patch_outputs(bundle_dir, response, session_id)
            
            # Check for snippet requests
            response_text = extract_response_text(response)
            should_continue, snippet_msg = check_and_handle_snippet_requests(
                queue_root, job, "patch", job_path, bundle_dir, response_text
            )
            
            if not should_continue:
                # Snippet follow-up was enqueued
                return True, "snippet_followup"
            
            # No snippet requests, check if patch was valid
            if success:
                log(queue_root, "INFO", f"wrote patch.diff to {bundle_dir}/analysis/")
                # Auto-enqueue apply job
                apply_job_path = enqueue_apply_job(queue_root, job, job_path.name)
                log(queue_root, "INFO", f"auto-enqueued apply job: {apply_job_path.name}")
                return True, "done"
            else:
                log(queue_root, "ERROR", f"patch validation failed: {error}")
                write_error_note(job_path, error)
                return False, error
            
        except (urllib.error.URLError, urllib.error.HTTPError, OSError, RuntimeError) as e:
            last_error = str(e)
            log(queue_root, "WARN", f"attempt {attempt + 1} failed: {last_error}")
            
            if attempt < max_retries - 1:
                delay = min(retry_delay * (2 ** attempt), 60)
                log(queue_root, "INFO", f"retrying in {delay}s...")
                time.sleep(delay)
    
    write_error_note(job_path, last_error or "unknown error")
    return False, last_error or "unknown error"


def process_apply_job(
    queue_root: Path,
    job_path: Path,
    job: dict,
    bundle_dir: Path,
) -> tuple[bool, str]:
    """
    Process an apply job by calling apply-patch script.
    Returns (success, status_message).
    
    The apply-patch script handles:
    - Applying the patch to a safe clone
    - Syncing to DPorts and running dsynth rebuild
    - Creating a PR if rebuild succeeds
    """
    # Check iteration limits
    iteration = int(job.get("iteration", "1"))
    max_iterations = int(job.get("max_iterations", str(DEFAULT_MAX_ITERATIONS)))
    
    if iteration > max_iterations:
        log(queue_root, "WARN", f"max iterations ({max_iterations}) reached for {job.get('origin')}, giving up")
        return False, "max_iterations_reached"
    
    log(queue_root, "INFO", f"apply job iteration {iteration}/{max_iterations} for {job.get('origin')}")
    
    # Find apply-patch script (same directory as this script)
    script_dir = Path(__file__).resolve().parent
    apply_script = script_dir / "apply-patch"
    
    if not apply_script.exists():
        log(queue_root, "ERROR", f"apply-patch script not found at {apply_script}")
        return False, "apply-patch script not found"
    
    # Check that patch.diff exists
    patch_file = bundle_dir / "analysis" / "patch.diff"
    if not patch_file.exists():
        log(queue_root, "ERROR", f"patch.diff not found in {bundle_dir}/analysis/")
        return False, "patch.diff not found"
    
    # Build command
    cmd = [str(apply_script), "--bundle", str(bundle_dir)]
    
    log(queue_root, "INFO", f"running: {' '.join(cmd)}")
    
    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=600  # 10 minute timeout for rebuild
        )
        
        # Log output
        if result.stdout:
            for line in result.stdout.strip().split("\n")[-20:]:  # Last 20 lines
                log(queue_root, "DEBUG", f"apply-patch: {line}")
        if result.stderr:
            for line in result.stderr.strip().split("\n")[-20:]:
                log(queue_root, "DEBUG", f"apply-patch: {line}")
        
        if result.returncode == 0:
            # Success! Check for PR URL
            pr_url_file = bundle_dir / "analysis" / "pr_url.txt"
            if pr_url_file.exists():
                pr_url = pr_url_file.read_text().strip()
                log(queue_root, "INFO", f"PR created: {pr_url}")
            else:
                log(queue_root, "INFO", "apply-patch succeeded (no PR URL found)")
            
            return True, "done"
        else:
            # Rebuild failed - the hook will create a new evidence bundle
            # which will start a new triage cycle (if within iteration limit)
            error_msg = result.stderr[:200] if result.stderr else f"exit code {result.returncode}"
            log(queue_root, "WARN", f"apply-patch failed: {error_msg}")
            
            # Write iteration tracking for the hook to pick up
            tracking_dir = bundle_dir.parent.parent  # Go up to runs/ level
            tracking_file = tracking_dir / ".current_apply_context"
            try:
                tracking_content = [
                    f"previous_bundle={bundle_dir}",
                    f"iteration={iteration}",
                    f"origin={job.get('origin', '')}",
                ]
                tracking_file.write_text("\n".join(tracking_content) + "\n")
                log(queue_root, "DEBUG", f"wrote tracking context to {tracking_file}")
            except OSError as e:
                log(queue_root, "WARN", f"failed to write tracking context: {e}")
            
            return False, error_msg
            
    except subprocess.TimeoutExpired:
        log(queue_root, "ERROR", "apply-patch timed out after 600s")
        return False, "timeout"
    except Exception as e:
        log(queue_root, "ERROR", f"apply-patch error: {e}")
        return False, str(e)


def process_job(
    queue_root: Path,
    job_path: Path,
    opencode_url: str,
    opencode_provider: str | None,
    opencode_model: str | None,
    timeout: int,
    max_retries: int,
    retry_delay: int,
    dry_run: bool,
    kedb_dir: Path | None
):
    """Process a single job (dispatch based on type)."""
    log(queue_root, "INFO", f"processing job {job_path.name}")
    
    job = parse_job_file(job_path)
    job_type = job.get("type", "triage")
    bundle_dir = Path(job.get("bundle_dir", ""))
    snippet_round = job.get("snippet_round", "0")
    
    if not bundle_dir.exists():
        log(queue_root, "ERROR", f"bundle_dir does not exist: {bundle_dir}")
        write_error_note(job_path, f"bundle_dir does not exist: {bundle_dir}")
        move_job(job_path, "failed")
        return
    
    if dry_run:
        if job_type == "apply":
            log(queue_root, "INFO", f"[dry-run] type=apply, would run apply-patch --bundle {bundle_dir}")
            print("=" * 60)
            print(f"JOB TYPE: apply (iteration={job.get('iteration', '1')})")
            print(f"BUNDLE: {bundle_dir}")
            print(f"ORIGIN: {job.get('origin', 'unknown')}")
            print("=" * 60)
            move_job(job_path, "pending")
            return
        elif job_type == "patch":
            payload = build_patch_payload(bundle_dir, kedb_dir, job)
        else:
            payload = build_triage_payload(bundle_dir, kedb_dir, job)
        
        log(queue_root, "INFO", f"[dry-run] type={job_type}, round={snippet_round}, would send payload ({len(payload)} bytes)")
        print("=" * 60)
        print(f"JOB TYPE: {job_type} (snippet_round={snippet_round})")
        print("=" * 60)
        print(payload)
        print("=" * 60)
        move_job(job_path, "pending")
        return
    
    # Dispatch based on job type
    if job_type == "apply":
        success, status = process_apply_job(
            queue_root, job_path, job, bundle_dir
        )
    elif job_type == "patch":
        success, status = process_patch_job(
            queue_root, job_path, job, bundle_dir,
            opencode_url, opencode_provider, opencode_model,
            timeout, max_retries, retry_delay, kedb_dir
        )
    else:
        success, status = process_triage_job(
            queue_root, job_path, job, bundle_dir,
            opencode_url, opencode_provider, opencode_model,
            timeout, max_retries, retry_delay, kedb_dir
        )
    
    if success:
        move_job(job_path, "done")
        if status == "snippet_followup":
            log(queue_root, "INFO", "moved job to done/ (snippet follow-up enqueued)")
        else:
            log(queue_root, "INFO", "moved job to done/")
    else:
        move_job(job_path, "failed")
        log(queue_root, "ERROR", f"moved job to failed/ ({status})")


# -----------------------------------------------------------------------------
# Main
# -----------------------------------------------------------------------------

def main():
    parser = argparse.ArgumentParser(description="Process dsynth failure jobs via opencode serve")
    parser.add_argument("--queue-root", required=True, help="Path to queue directory")
    parser.add_argument("--once", action="store_true", help="Process one job and exit")
    parser.add_argument("--dry-run", action="store_true", help="Print payload without calling opencode")
    parser.add_argument("--kedb-dir", help="Path to KEDB directory (default: auto-detect)")
    args = parser.parse_args()
    
    queue_root = Path(args.queue_root)
    
    # Validate queue structure
    for subdir in ["pending", "inflight", "done", "failed"]:
        d = queue_root / subdir
        if not d.exists():
            print(f"error: queue directory missing: {d}", file=sys.stderr)
            sys.exit(1)
    
    # Find KEDB directory
    if args.kedb_dir:
        kedb_dir = Path(args.kedb_dir)
        if not kedb_dir.exists():
            print(f"warning: KEDB directory not found: {kedb_dir}", file=sys.stderr)
            kedb_dir = None
    else:
        kedb_dir = find_kedb_dir()
    
    # Get config from environment
    opencode_url = os.environ.get("OPENCODE_URL")
    if not opencode_url and not args.dry_run:
        print("error: OPENCODE_URL environment variable required", file=sys.stderr)
        sys.exit(1)
    
    opencode_provider = os.environ.get("OPENCODE_PROVIDER", "opencode")
    opencode_model = os.environ.get("OPENCODE_MODEL", "gpt-5-nano")
    timeout = int(os.environ.get("OPENCODE_TIMEOUT", "120"))
    max_retries = int(os.environ.get("OPENCODE_MAX_RETRIES", "3"))
    retry_delay = int(os.environ.get("OPENCODE_RETRY_DELAY", "8"))
    
    model_info = f"{opencode_provider}/{opencode_model}"
    kedb_info = str(kedb_dir) if kedb_dir else "none"
    log(queue_root, "INFO", f"starting runner (once={args.once}, dry_run={args.dry_run}, model={model_info}, kedb={kedb_info})")
    
    if args.once:
        job = claim_next_job(queue_root)
        if job:
            process_job(
                queue_root, job, opencode_url,
                opencode_provider, opencode_model,
                timeout, max_retries, retry_delay,
                args.dry_run, kedb_dir
            )
        else:
            log(queue_root, "INFO", "no jobs in queue")
    else:
        while True:
            job = claim_next_job(queue_root)
            if job:
                process_job(
                    queue_root, job, opencode_url,
                    opencode_provider, opencode_model,
                    timeout, max_retries, retry_delay,
                    args.dry_run, kedb_dir
                )
            else:
                time.sleep(5)


if __name__ == "__main__":
    main()
