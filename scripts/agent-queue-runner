#!/usr/bin/env python3
"""
agent-queue-runner: Process dsynth failure jobs via opencode serve.

Usage:
    agent-queue-runner --queue-root <path> [--once] [--dry-run]

Environment variables:
    OPENCODE_URL              (required) e.g., http://192.168.1.10:4096
    OPENCODE_PROVIDER         (optional) e.g., opencode (default: opencode for dports-* agents)
    OPENCODE_MODEL            (optional) e.g., gpt-5-nano (default: gpt-5-nano for dports-* agents)
    OPENCODE_TIMEOUT          (optional) request timeout in seconds, default 120
    OPENCODE_MAX_RETRIES      (optional) retry attempts, default 3
    OPENCODE_RETRY_DELAY      (optional) base delay between retries, default 8
    OPENCODE_MAX_SNIPPET_ROUNDS (optional) max snippet extraction rounds, default 5
    APPLY_PATCH_FLAGS         (optional) extra flags for apply-patch, default "--no-push --no-pr"

    VM_SSH_HOST               (optional) SSH host for remote snippet extraction
                              If set, snippet-extractor runs via SSH (for Linux dev hosts)
                              If not set, snippet-extractor runs locally (DragonFlyBSD native)
    VM_SSH_KEY                (optional) SSH key for VM access (only used if VM_SSH_HOST set)
    VM_SSH_PORT               (optional) SSH port (only used if VM_SSH_HOST set), default 2222

Job types:
    type=triage (default) - Uses dports-triage agent, may auto-enqueue patch job
    type=patch            - Uses dports-patch agent to generate a diff
    type=apply            - Applies patch, rebuilds, creates PR if successful

Job fields:
    snippet_round=N       - Current snippet round (0 = initial, no snippets)
    has_snippets=true     - Snippets are available in bundle
    parent_job=...        - Parent job filename (for tracking lineage)
    iteration=N           - Current fix iteration (1-based, for apply jobs)
    max_iterations=N      - Maximum iterations before giving up (default: 3)
    previous_bundle=...   - Bundle from previous failed attempt (if iteration > 1)
"""

import argparse
import difflib
import json
import os
import re
import shlex
import sqlite3
import subprocess
import sys
import threading
import time
import urllib.request
import urllib.error
from datetime import datetime, timezone
from pathlib import Path


# Classifications that should trigger automatic patch generation
PATCHABLE_CLASSIFICATIONS = {
    "compile-error",
    "configure-error",
    "patch-error",
    "plist-error",
}

# Confidence levels that allow patch generation
PATCHABLE_CONFIDENCE = {"high", "medium"}

# Max snippet escalation rounds (default, can override with env var)
DEFAULT_MAX_SNIPPET_ROUNDS = 5

# Max fix iterations before giving up on a port
DEFAULT_MAX_ITERATIONS = 3

# VM SSH defaults
DEFAULT_VM_SSH_KEY = "/home/antonioh/.go-synth/vm/id_ed25519"
DEFAULT_VM_SSH_PORT = "2222"
DEFAULT_VM_SSH_HOST = "root@localhost"

# Heartbeat interval (seconds)
HEARTBEAT_INTERVAL = 5


# =============================================================================
# State DB connection (for activity logging and runner status)
# =============================================================================

_state_db_conn: sqlite3.Connection | None = None
_state_db_lock = threading.Lock()
_heartbeat_stop_event = threading.Event()
_heartbeat_thread: threading.Thread | None = None
_current_job_id: str | None = None
_current_stage: str | None = None


def get_state_db_path(queue_root: Path) -> Path:
    """Get path to state.db (same directory as queue)."""
    # Queue is at <logs>/evidence/queue/, state.db is at <logs>/evidence/state.db
    return queue_root.parent / "state.db"


def init_state_db(queue_root: Path) -> sqlite3.Connection | None:
    """Initialize connection to state.db for activity logging."""
    global _state_db_conn
    
    db_path = get_state_db_path(queue_root)
    
    if not db_path.exists():
        # State server hasn't created the DB yet - that's ok
        return None
    
    try:
        conn = sqlite3.connect(str(db_path), check_same_thread=False)
        conn.row_factory = sqlite3.Row
        _state_db_conn = conn
        return conn
    except Exception as e:
        print(f"Warning: Could not connect to state.db: {e}", file=sys.stderr)
        return None


def activity_log(
    queue_root: Path,
    stage: str,
    message: str,
    job_id: str | None = None,
    duration_ms: int | None = None,
    extra: dict | None = None
):
    """
    Log activity to state.db activity_log table.
    Also updates _current_stage for heartbeat.
    Keeps only last 10 entries.
    """
    global _current_stage
    _current_stage = stage
    
    # Also write to runner.log for backwards compatibility
    log(queue_root, "INFO", f"[{stage}] {message}")
    
    if _state_db_conn is None:
        return
    
    ts = datetime.now(timezone.utc).isoformat()
    extra_json = json.dumps(extra) if extra else None
    
    try:
        with _state_db_lock:
            _state_db_conn.execute(
                """INSERT INTO activity_log (ts, job_id, stage, message, duration_ms, extra_json)
                   VALUES (?, ?, ?, ?, ?, ?)""",
                (ts, job_id, stage, message, duration_ms, extra_json)
            )
            
            # Prune to keep only last 10 entries
            _state_db_conn.execute(
                """DELETE FROM activity_log WHERE id NOT IN (
                     SELECT id FROM activity_log ORDER BY id DESC LIMIT 10
                   )"""
            )
            
            _state_db_conn.commit()
    except Exception as e:
        print(f"Warning: Failed to write activity log: {e}", file=sys.stderr)


def update_runner_status(
    status: str,
    job_id: str | None = None,
    stage: str | None = None,
    extra: dict | None = None
):
    """Update runner_status table (singleton row)."""
    global _current_job_id, _current_stage
    
    _current_job_id = job_id
    if stage is not None:
        _current_stage = stage
    
    if _state_db_conn is None:
        return
    
    ts = datetime.now(timezone.utc).isoformat()
    extra_json = json.dumps(extra) if extra else None
    
    try:
        with _state_db_lock:
            # Upsert the singleton row
            _state_db_conn.execute(
                """INSERT INTO runner_status (id, status, job_id, current_stage, started_at, updated_at, extra_json)
                   VALUES (1, ?, ?, ?, ?, ?, ?)
                   ON CONFLICT(id) DO UPDATE SET
                     status = excluded.status,
                     job_id = excluded.job_id,
                     current_stage = excluded.current_stage,
                     started_at = CASE WHEN excluded.job_id != runner_status.job_id THEN excluded.started_at ELSE runner_status.started_at END,
                     updated_at = excluded.updated_at,
                     extra_json = excluded.extra_json""",
                (status, job_id, stage or _current_stage, ts, ts, extra_json)
            )
            _state_db_conn.commit()
    except Exception as e:
        print(f"Warning: Failed to update runner status: {e}", file=sys.stderr)


def _heartbeat_loop():
    """Background thread that updates runner_status.updated_at every 5 seconds."""
    while not _heartbeat_stop_event.is_set():
        if _state_db_conn is not None:
            try:
                ts = datetime.now(timezone.utc).isoformat()
                with _state_db_lock:
                    _state_db_conn.execute(
                        """UPDATE runner_status SET updated_at = ? WHERE id = 1""",
                        (ts,)
                    )
                    _state_db_conn.commit()
            except Exception:
                pass
        
        _heartbeat_stop_event.wait(HEARTBEAT_INTERVAL)


def start_heartbeat():
    """Start the heartbeat thread."""
    global _heartbeat_thread
    
    if _heartbeat_thread is not None:
        return
    
    _heartbeat_stop_event.clear()
    _heartbeat_thread = threading.Thread(target=_heartbeat_loop, daemon=True)
    _heartbeat_thread.start()


def stop_heartbeat():
    """Stop the heartbeat thread."""
    global _heartbeat_thread
    
    _heartbeat_stop_event.set()
    if _heartbeat_thread is not None:
        _heartbeat_thread.join(timeout=2)
        _heartbeat_thread = None


def log(queue_root: Path, level: str, message: str):
    """Log to both stderr and runner.log."""
    ts = datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")
    line = f"{ts} {level:5} {message}"
    print(line, file=sys.stderr)
    try:
        with open(queue_root / "runner.log", "a") as f:
            f.write(line + "\n")
    except OSError:
        pass


def parse_job_file(path: Path) -> dict:
    """Parse key=value job file into dict."""
    data = {}
    with open(path) as f:
        for line in f:
            line = line.strip()
            if "=" in line:
                key, _, value = line.partition("=")
                data[key] = value
    return data


def read_file_if_exists(path: Path, max_bytes: int = 200_000) -> str | None:
    """Read file contents if it exists, truncate if too large."""
    if not path.exists():
        return None
    try:
        content = path.read_text(errors="replace")
        if len(content) > max_bytes:
            content = content[:max_bytes] + "\n[...truncated...]\n"
        return content
    except OSError:
        return None


def find_kedb_dir() -> Path | None:
    """Find the KEDB directory relative to this script or in DeltaPorts repo."""
    script_dir = Path(__file__).resolve().parent
    kedb_dir = script_dir.parent / "docs" / "kedb"
    if kedb_dir.exists():
        return kedb_dir
    return None


def load_kedb(kedb_dir: Path | None) -> str:
    """Load all KEDB markdown files into a single context block."""
    if not kedb_dir or not kedb_dir.exists():
        return ""
    
    kedb_files = sorted(kedb_dir.glob("*.md"))
    skip_files = {"readme.md", "template.md"}
    kedb_files = [f for f in kedb_files if f.name.lower() not in skip_files]
    
    if not kedb_files:
        return ""
    
    parts = ["## Known Error Database (KEDB)", ""]
    parts.append("The following are known DragonFlyBSD-specific build issues and their fixes:")
    parts.append("")
    
    for kf in kedb_files:
        content = read_file_if_exists(kf, max_bytes=50_000)
        if content:
            parts.append(f"### {kf.stem}")
            parts.append(content)
            parts.append("")
    
    return "\n".join(parts)


# -----------------------------------------------------------------------------
# Triage parsing
# -----------------------------------------------------------------------------

def parse_triage_output(triage_path: Path) -> dict:
    """Extract Classification and Confidence from triage.md."""
    result = {"classification": "", "confidence": "", "raw": ""}
    
    content = read_file_if_exists(triage_path)
    if not content:
        return result
    
    result["raw"] = content
    
    # Extract Classification
    match = re.search(r"^##\s*Classification\s*\n+([^\n#]+)", content, re.MULTILINE | re.IGNORECASE)
    if match:
        result["classification"] = match.group(1).strip().lower()
    
    # Extract Confidence
    match = re.search(r"^##\s*Confidence\s*\n+([^\n#]+)", content, re.MULTILINE | re.IGNORECASE)
    if match:
        result["confidence"] = match.group(1).strip().lower()
    
    return result


def should_enqueue_patch(triage: dict) -> bool:
    """Determine if a patch job should be auto-enqueued based on triage results."""
    classification = triage.get("classification", "")
    confidence = triage.get("confidence", "")
    
    if classification not in PATCHABLE_CLASSIFICATIONS:
        return False
    if confidence not in PATCHABLE_CONFIDENCE:
        return False
    return True


# -----------------------------------------------------------------------------
# Snippet escalation
# -----------------------------------------------------------------------------

def parse_snippet_requests(content: str) -> list[str]:
    """Parse ## Snippet Requests section from markdown, return raw request lines."""
    match = re.search(
        r"^##\s*Snippet Requests\s*\n(.*?)(?=^##|\Z)",
        content,
        re.MULTILINE | re.DOTALL | re.IGNORECASE
    )
    if not match:
        return []
    
    section = match.group(1)
    requests = []
    
    for line in section.split("\n"):
        line = line.strip()
        if not line:
            continue
        # Parse: - `request:spec` â€” description
        m = re.match(r"-\s*`([^`]+)`", line)
        if m:
            requests.append(m.group(1))
    
    return requests


def get_vm_ssh_command() -> list[str]:
    """Build base SSH command for VM access."""
    ssh_key = os.environ.get("VM_SSH_KEY", DEFAULT_VM_SSH_KEY)
    ssh_port = os.environ.get("VM_SSH_PORT", DEFAULT_VM_SSH_PORT)
    ssh_host = os.environ.get("VM_SSH_HOST", DEFAULT_VM_SSH_HOST)
    
    return [
        "ssh",
        "-i", ssh_key,
        "-p", ssh_port,
        "-o", "StrictHostKeyChecking=no",
        "-o", "UserKnownHostsFile=/dev/null",
        "-o", "BatchMode=yes",
        "-o", "ConnectTimeout=10",
        ssh_host,
    ]


def run_snippet_extractor(
    queue_root: Path,
    bundle_dir: Path,
    round_num: int,
    prefer_workdir: bool = True
) -> tuple[bool, str]:
    """
    Run snippet-extractor, either locally or via SSH to VM.
    
    Mode selection:
    - If VM_SSH_HOST is set: SSH to VM and run remotely
    - Otherwise: Run snippet-extractor directly on local system
    
    Returns (success, error_message).
    """
    # Build extractor arguments
    extractor_args = [
        "--bundle", str(bundle_dir),
        "--round", str(round_num),
    ]
    if prefer_workdir:
        extractor_args.append("--prefer-workdir")
    
    # Determine execution mode based on VM_SSH_HOST presence
    vm_ssh_host = os.environ.get("VM_SSH_HOST")
    
    if vm_ssh_host:
        # SSH mode: run on remote VM
        ssh_cmd = get_vm_ssh_command()
        full_cmd = ssh_cmd + ["/usr/local/bin/snippet-extractor"] + extractor_args
        mode_desc = f"on VM ({vm_ssh_host})"
    else:
        # Local mode: run directly
        # Try to find snippet-extractor in common locations
        extractor_path = None
        candidates = [
            "/usr/local/bin/snippet-extractor",
            str(Path(__file__).parent / "snippet-extractor"),
        ]
        for candidate in candidates:
            if Path(candidate).exists():
                extractor_path = candidate
                break
        
        if not extractor_path:
            return False, "snippet-extractor not found in expected locations"
        
        full_cmd = [extractor_path] + extractor_args
        mode_desc = "locally"
    
    log(queue_root, "INFO", f"running snippet-extractor round {round_num} {mode_desc}")
    
    try:
        result = subprocess.run(
            full_cmd,
            capture_output=True,
            text=True,
            timeout=120
        )
        
        # Log output
        if result.stdout:
            for line in result.stdout.strip().split("\n"):
                log(queue_root, "DEBUG", f"extractor: {line}")
        if result.stderr:
            for line in result.stderr.strip().split("\n"):
                log(queue_root, "DEBUG", f"extractor: {line}")
        
        if result.returncode == 0:
            log(queue_root, "INFO", f"snippet extraction round {round_num} succeeded")
            return True, ""
        elif result.returncode == 1:
            return False, "No snippet requests found"
        elif result.returncode == 2:
            return False, "All snippet requests failed"
        else:
            return False, f"Extractor exit code {result.returncode}"
            
    except subprocess.TimeoutExpired:
        return False, "Snippet extraction timed out"
    except Exception as e:
        error_type = "SSH/extractor" if vm_ssh_host else "Extractor"
        return False, f"{error_type} error: {e}"


def build_snippet_feedback(bundle_dir: Path, round_num: int) -> str:
    """Generate feedback section from snippet manifest for agent context."""
    manifest_path = bundle_dir / "analysis" / "snippets" / "manifest.json"
    if not manifest_path.exists():
        return ""
    
    try:
        with open(manifest_path) as f:
            manifest = json.load(f)
    except Exception:
        return ""
    
    rounds = manifest.get("rounds", [])
    if not rounds:
        return ""
    
    # Find the latest round
    latest_round = None
    for r in rounds:
        if r.get("round") == round_num:
            latest_round = r
            break
    
    if not latest_round:
        # Use the last round
        latest_round = rounds[-1]
    
    parts = ["## Snippet Extraction Results", ""]
    parts.append(f"**Round {latest_round.get('round', '?')}** | Source: `{latest_round.get('source', 'unknown')}` | Budget remaining: {latest_round.get('budget_remaining', 0)} bytes")
    parts.append("")
    
    requests = latest_round.get("requests", [])
    if requests:
        parts.append("| Request | Status | Output | Bytes |")
        parts.append("|---------|--------|--------|-------|")
        for req in requests:
            raw = req.get("raw", "?")[:40]
            status = req.get("status", "?")
            output = req.get("output", "-")
            if output and len(output) > 30:
                output = "..." + output[-27:]
            bytes_ = req.get("bytes", 0)
            note = req.get("note", "")
            
            # Add emoji for status
            status_display = {
                "ok": "ok",
                "not_found": "not_found",
                "budget_exceeded": "budget_exceeded",
                "empty": "empty",
            }.get(status, status)
            
            parts.append(f"| `{raw}` | {status_display} | {output or '-'} | {bytes_} |")
            if note:
                parts.append(f"|  | *{note}* | | |")
    
    parts.append("")
    
    # Add summary
    total_rounds = manifest.get("total_rounds", 0)
    max_rounds = int(os.environ.get("OPENCODE_MAX_SNIPPET_ROUNDS", DEFAULT_MAX_SNIPPET_ROUNDS))
    remaining_rounds = max_rounds - total_rounds
    
    parts.append(f"**Snippet rounds used:** {total_rounds}/{max_rounds} (remaining: {remaining_rounds})")
    if remaining_rounds <= 0:
        parts.append("**NOTE:** No more snippet rounds available. Work with the information provided.")
    parts.append("")
    
    return "\n".join(parts)


def load_snippets_content(bundle_dir: Path, round_num: int, max_bytes: int = 200_000) -> str:
    """Load extracted snippet contents for inclusion in payload."""
    round_dir = bundle_dir / "analysis" / "snippets" / f"round_{round_num}"
    if not round_dir.exists():
        return ""
    
    parts = ["## Extracted Snippets", ""]
    total_bytes = 0
    
    # Load round manifest for context
    manifest_path = round_dir / "manifest.json"
    if manifest_path.exists():
        try:
            with open(manifest_path) as f:
                round_manifest = json.load(f)
            source_type = round_manifest.get("source", "unknown")
            distfile = round_manifest.get("distfile")
            if distfile:
                parts.append(f"*Source: distfile `{distfile}`*")
            else:
                parts.append(f"*Source: {source_type}*")
            parts.append("")
        except Exception:
            pass
    
    # Walk through subdirectories (source, buildsystem, configure, log)
    for subdir in sorted(round_dir.iterdir()):
        if not subdir.is_dir() or subdir.name.startswith("."):
            continue
        
        for file_path in sorted(subdir.glob("*.txt")):
            if total_bytes >= max_bytes:
                parts.append(f"*[...truncated, budget exceeded...]*")
                break
            
            try:
                content = file_path.read_text(errors="replace")
                remaining = max_bytes - total_bytes
                if len(content) > remaining:
                    content = content[:remaining] + "\n[...truncated...]\n"
                
                # Infer original filename from safe name
                original_name = file_path.stem.replace("_", "/")
                
                parts.append(f"### {subdir.name}/{original_name}")
                parts.append("```")
                parts.append(content)
                parts.append("```")
                parts.append("")
                
                total_bytes += len(content)
            except Exception:
                continue
        
        if total_bytes >= max_bytes:
            break
    
    if total_bytes == 0:
        return ""
    
    return "\n".join(parts)


def enqueue_followup_job(
    queue_root: Path,
    job: dict,
    job_type: str,
    new_round: int,
    parent_job_name: str
) -> Path:
    """Create a follow-up job with incremented snippet round."""
    ts = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%SZ")
    origin_safe = job.get("origin", "unknown").replace("/", "_")
    pid = os.getpid()
    
    job_name = f"{ts}-{job.get('profile', 'unknown')}-{origin_safe}-{pid}-{job_type}-r{new_round}.job"
    
    pending_dir = queue_root / "pending"
    job_path = pending_dir / job_name
    
    content = [
        f"type={job_type}",
        f"created_ts_utc={ts}",
        f"profile={job.get('profile', '')}",
        f"origin={job.get('origin', '')}",
        f"flavor={job.get('flavor', '')}",
        f"bundle_dir={job.get('bundle_dir', '')}",
        f"run_id={job.get('run_id', '')}",
        f"snippet_round={new_round}",
        f"has_snippets=true",
        f"parent_job={parent_job_name}",
    ]
    
    # Preserve triage_file for patch jobs
    if job_type == "patch":
        triage_file = job.get("triage_file", f"{job.get('bundle_dir', '')}/analysis/triage.md")
        content.append(f"triage_file={triage_file}")
    
    # Atomic write
    tmp_path = job_path.with_suffix(".tmp")
    with open(tmp_path, "w") as f:
        f.write("\n".join(content) + "\n")
    tmp_path.rename(job_path)
    
    return job_path


def check_and_handle_snippet_requests(
    queue_root: Path,
    job: dict,
    job_type: str,
    job_path: Path,
    bundle_dir: Path,
    response_text: str
) -> tuple[bool, str]:
    """
    Check if agent requested snippets and handle extraction.
    Returns (should_continue_processing, message).
    
    If snippets were requested and we haven't hit max rounds:
    - Run extractor on VM
    - Enqueue follow-up job
    - Return (False, "enqueued follow-up")
    
    Otherwise return (True, "") to continue normal processing.
    """
    # Parse snippet requests from response
    requests = parse_snippet_requests(response_text)
    if not requests:
        return True, ""
    
    current_round = int(job.get("snippet_round", "0"))
    max_rounds = int(os.environ.get("OPENCODE_MAX_SNIPPET_ROUNDS", DEFAULT_MAX_SNIPPET_ROUNDS))
    
    if current_round >= max_rounds:
        log(queue_root, "WARN", f"snippet requests found but max rounds ({max_rounds}) reached")
        return True, ""
    
    new_round = current_round + 1
    log(queue_root, "INFO", f"found {len(requests)} snippet requests, starting round {new_round}")
    
    # Run extractor on VM
    success, error = run_snippet_extractor(queue_root, bundle_dir, new_round)
    if not success:
        log(queue_root, "WARN", f"snippet extraction failed: {error}")
        # Continue processing anyway - agent will work without snippets
        return True, ""
    
    # Enqueue follow-up job
    followup_path = enqueue_followup_job(
        queue_root, job, job_type, new_round, job_path.name
    )
    log(queue_root, "INFO", f"enqueued follow-up job: {followup_path.name}")
    
    return False, f"enqueued snippet round {new_round}"


# -----------------------------------------------------------------------------
# File content extraction and diff generation
# -----------------------------------------------------------------------------

def extract_files_from_response(text: str) -> dict[str, str]:
    """
    Extract file content blocks from agent response.
    
    Expected format:
    ### FILE: ports/category/port/filename
    ```<language>
    <content>
    ```
    
    Returns dict of {filepath: content}.
    """
    files = {}
    
    # Pattern: ### FILE: <path> followed by code block
    # This handles multiple files in sequence
    pattern = r"###\s*FILE:\s*([^\n]+)\n```[^\n]*\n(.*?)```"
    
    for match in re.finditer(pattern, text, re.DOTALL):
        filepath = match.group(1).strip()
        content = match.group(2)
        
        # Normalize path - remove leading slashes or DeltaPorts prefix
        filepath = filepath.lstrip("/")
        if filepath.startswith("DeltaPorts/"):
            filepath = filepath[len("DeltaPorts/"):]
        
        # Content should not have trailing newline stripped (preserve exact content)
        # But remove any leading/trailing whitespace from the match
        content = content.rstrip("\n")
        
        if filepath and content:
            files[filepath] = content
    
    return files


def generate_unified_diff(filepath: str, old_content: str | None, new_content: str) -> str:
    """
    Generate a unified diff for a file.
    
    Args:
        filepath: Path relative to DeltaPorts root (e.g., ports/net/foo/Makefile.DragonFly)
        old_content: Original file content (None if new file)
        new_content: New file content
    
    Returns:
        Unified diff string
    """
    import difflib
    
    if old_content is None:
        # New file: use /dev/null as source
        old_lines = []
        old_path = "/dev/null"
        new_path = f"b/{filepath}"
    else:
        old_lines = old_content.splitlines(keepends=True)
        # Ensure last line has newline for proper diff
        if old_lines and not old_lines[-1].endswith("\n"):
            old_lines[-1] += "\n"
        old_path = f"a/{filepath}"
        new_path = f"b/{filepath}"
    
    new_lines = new_content.splitlines(keepends=True)
    # Ensure last line has newline
    if new_lines and not new_lines[-1].endswith("\n"):
        new_lines[-1] += "\n"
    
    diff_lines = list(difflib.unified_diff(
        old_lines,
        new_lines,
        fromfile=old_path,
        tofile=new_path,
    ))
    
    return "".join(diff_lines)


def get_existing_file_content(bundle_dir: Path, filepath: str) -> str | None:
    """
    Get existing file content from bundle if it exists.
    
    Checks bundle_dir/port/Makefile.DragonFly or similar locations.
    """
    # Extract the filename from the full path
    # filepath is like: ports/net/hostapd/Makefile.DragonFly
    parts = filepath.split("/")
    
    if len(parts) >= 4 and parts[0] == "ports":
        # Extract filename (last part)
        filename = parts[-1]
        
        # Check in bundle_dir/port/
        candidate = bundle_dir / "port" / filename
        if candidate.exists():
            try:
                return candidate.read_text()
            except OSError:
                pass
    
    return None


def generate_combined_diff(
    bundle_dir: Path,
    files: dict[str, str]
) -> str:
    """
    Generate a combined unified diff for all files.
    
    Args:
        bundle_dir: Evidence bundle directory
        files: Dict of {filepath: new_content}
    
    Returns:
        Combined unified diff string
    """
    diffs = []
    
    for filepath, new_content in sorted(files.items()):
        # Check if file exists in bundle
        old_content = get_existing_file_content(bundle_dir, filepath)
        
        diff = generate_unified_diff(filepath, old_content, new_content)
        if diff:
            diffs.append(diff)
    
    return "\n".join(diffs)


# -----------------------------------------------------------------------------
# Diff validation (for legacy format compatibility)
# -----------------------------------------------------------------------------

def validate_diff(diff_text: str) -> tuple[bool, str]:
    """
    Validate unified diff syntax.
    Returns (is_valid, error_message).
    """
    if not diff_text or not diff_text.strip():
        return False, "Empty diff"
    
    lines = diff_text.strip().split("\n")
    
    # Must have --- and +++ headers
    has_old_file = any(line.startswith("---") for line in lines)
    has_new_file = any(line.startswith("+++") for line in lines)
    
    if not has_old_file:
        return False, "Missing '---' file header"
    if not has_new_file:
        return False, "Missing '+++' file header"
    
    # Must have at least one hunk header
    has_hunk = any(line.startswith("@@") for line in lines)
    if not has_hunk:
        return False, "Missing '@@ ... @@' hunk header"
    
    # Check that lines in hunks have valid prefixes
    in_hunk = False
    for line in lines:
        if line.startswith("@@"):
            in_hunk = True
            continue
        if in_hunk:
            # End of hunk detection (next file or end)
            if line.startswith("---") or line.startswith("+++") or line.startswith("diff "):
                in_hunk = False
                continue
            # Valid hunk line prefixes: +, -, space, or empty (for context)
            if line and not line[0] in ("+", "-", " ", "\\"):
                # Allow "\ No newline at end of file"
                if not line.startswith("\\"):
                    return False, f"Invalid line prefix in hunk: {line[:50]}"
    
    return True, ""


def extract_diff_from_response(text: str) -> str | None:
    """Extract the diff block from the agent's response."""
    # Look for ```diff ... ``` block
    match = re.search(r"```diff\s*\n(.*?)```", text, re.DOTALL)
    if match:
        return match.group(1).strip()
    
    # Fallback: look for content starting with "---" or "diff --git"
    lines = text.split("\n")
    diff_start = None
    for i, line in enumerate(lines):
        if line.startswith("---") or line.startswith("diff --git") or line.startswith("diff -"):
            diff_start = i
            break
    
    if diff_start is not None:
        # Take everything from diff start until we hit a section marker
        # Diff ends when we see text that's clearly not part of a diff
        diff_lines = []
        in_hunk = False
        for line in lines[diff_start:]:
            # Stop on markdown headers
            if line.startswith("## ") and not line.startswith("## Patch"):
                break
            # Stop on common section markers (without ## prefix)
            if line.strip() in ("Rationale", "Files Modified", "Testing Notes", "Snippet Requests"):
                break
            # Stop if line looks like prose after a hunk (starts with capital, no diff prefix)
            if in_hunk and line and not line[0] in " +-@\\":
                # Check if it looks like a new file header or prose
                if not line.startswith("---") and not line.startswith("+++") and not line.startswith("diff"):
                    # Likely end of diff - prose text
                    if len(line) > 10 and line[0].isupper() and " " in line[:20]:
                        break
            if line.startswith("@@"):
                in_hunk = True
            diff_lines.append(line)
        return "\n".join(diff_lines).strip()
    
    return None


# -----------------------------------------------------------------------------
# Payload builders
# -----------------------------------------------------------------------------

def build_triage_payload(
    bundle_dir: Path,
    kedb_dir: Path | None = None,
    job: dict | None = None
) -> str:
    """Build the triage prompt from bundle contents."""
    parts = []
    
    # Include snippet feedback and content if this is a follow-up round
    snippet_round = int(job.get("snippet_round", "0")) if job else 0
    has_snippets = job.get("has_snippets", "false") == "true" if job else False
    
    if has_snippets and snippet_round > 0:
        # Add feedback about previous extraction
        feedback = build_snippet_feedback(bundle_dir, snippet_round)
        if feedback:
            parts.append(feedback)
            parts.append("")
        
        # Add extracted snippet contents
        snippet_content = load_snippets_content(bundle_dir, snippet_round)
        if snippet_content:
            parts.append(snippet_content)
            parts.append("")
    
    # Known Error Database (if available)
    kedb_content = load_kedb(kedb_dir)
    if kedb_content:
        parts.append(kedb_content)
        parts.append("")
    
    # Metadata
    meta = read_file_if_exists(bundle_dir / "meta.txt")
    if meta:
        parts.append("## Metadata")
        parts.append(meta)
        parts.append("")
    
    # Build errors
    errors = read_file_if_exists(bundle_dir / "logs" / "errors.txt")
    if errors:
        parts.append("## Build Errors")
        parts.append(errors)
        parts.append("")
    
    # Port files
    parts.append("## Port Files")
    
    makefile = read_file_if_exists(bundle_dir / "port" / "Makefile")
    if makefile:
        parts.append("### Makefile")
        parts.append("```makefile")
        parts.append(makefile)
        parts.append("```")
        parts.append("")
    
    plist = read_file_if_exists(bundle_dir / "port" / "pkg-plist")
    if plist:
        parts.append("### pkg-plist")
        parts.append("```")
        parts.append(plist)
        parts.append("```")
        parts.append("")
    
    distinfo = read_file_if_exists(bundle_dir / "port" / "distinfo")
    if distinfo:
        parts.append("### distinfo")
        parts.append("```")
        parts.append(distinfo)
        parts.append("```")
        parts.append("")
    
    # Patches
    patches_dir = bundle_dir / "port" / "files"
    if patches_dir.exists():
        patch_files = sorted(patches_dir.glob("patch-*"))
        if patch_files:
            parts.append("### Existing Patches")
            for pf in patch_files:
                content = read_file_if_exists(pf)
                if content:
                    parts.append(f"#### {pf.name}")
                    parts.append("```diff")
                    parts.append(content)
                    parts.append("```")
                    parts.append("")
    
    parts.append("---")
    parts.append("Analyze this build failure and provide your triage report.")
    
    return "\n".join(parts)


def build_patch_payload(
    bundle_dir: Path,
    kedb_dir: Path | None = None,
    job: dict | None = None
) -> str:
    """Build the patch generation prompt including triage output."""
    parts = []
    
    # Include snippet feedback and content if this is a follow-up round
    snippet_round = int(job.get("snippet_round", "0")) if job else 0
    has_snippets = job.get("has_snippets", "false") == "true" if job else False
    
    if has_snippets and snippet_round > 0:
        # Add feedback about previous extraction
        feedback = build_snippet_feedback(bundle_dir, snippet_round)
        if feedback:
            parts.append(feedback)
            parts.append("")
        
        # Add extracted snippet contents
        snippet_content = load_snippets_content(bundle_dir, snippet_round)
        if snippet_content:
            parts.append(snippet_content)
            parts.append("")
    
    # Triage summary (most important context)
    triage = read_file_if_exists(bundle_dir / "analysis" / "triage.md")
    if triage:
        parts.append("## Triage Summary")
        parts.append(triage)
        parts.append("")
    
    # Known Error Database (if available)
    kedb_content = load_kedb(kedb_dir)
    if kedb_content:
        parts.append(kedb_content)
        parts.append("")
    
    # Metadata
    meta = read_file_if_exists(bundle_dir / "meta.txt")
    if meta:
        parts.append("## Metadata")
        parts.append(meta)
        parts.append("")
    
    # Build errors
    errors = read_file_if_exists(bundle_dir / "logs" / "errors.txt")
    if errors:
        parts.append("## Build Errors")
        parts.append(errors)
        parts.append("")
    
    # Port files
    parts.append("## Port Files")
    
    makefile = read_file_if_exists(bundle_dir / "port" / "Makefile")
    if makefile:
        parts.append("### Makefile")
        parts.append("```makefile")
        parts.append(makefile)
        parts.append("```")
        parts.append("")
    
    plist = read_file_if_exists(bundle_dir / "port" / "pkg-plist")
    if plist:
        parts.append("### pkg-plist")
        parts.append("```")
        parts.append(plist)
        parts.append("```")
        parts.append("")
    
    distinfo = read_file_if_exists(bundle_dir / "port" / "distinfo")
    if distinfo:
        parts.append("### distinfo")
        parts.append("```")
        parts.append(distinfo)
        parts.append("```")
        parts.append("")
    
    # Existing patches
    patches_dir = bundle_dir / "port" / "files"
    if patches_dir.exists():
        patch_files = sorted(patches_dir.glob("patch-*"))
        if patch_files:
            parts.append("### Existing Patches")
            for pf in patch_files:
                content = read_file_if_exists(pf)
                if content:
                    parts.append(f"#### {pf.name}")
                    parts.append("```diff")
                    parts.append(content)
                    parts.append("```")
                    parts.append("")
    
    # Check if Makefile.DragonFly exists in bundle
    makefile_dfly = read_file_if_exists(bundle_dir / "port" / "Makefile.DragonFly")
    has_makefile_dfly = makefile_dfly is not None
    
    if has_makefile_dfly:
        parts.append("### Makefile.DragonFly (existing - modify this file)")
        parts.append("```makefile")
        parts.append(makefile_dfly)
        parts.append("```")
        parts.append("")
    else:
        parts.append("### Makefile.DragonFly")
        parts.append("**This file does not exist yet. You must CREATE it.**")
        parts.append("")
    
    parts.append("---")
    
    # Build instructions with context about whether file exists
    if has_makefile_dfly:
        file_instruction = """
IMPORTANT: Makefile.DragonFly already exists. Your diff should MODIFY the existing file.
Use `--- a/ports/...` and `+++ b/ports/...` format with proper context lines."""
    else:
        file_instruction = """
IMPORTANT: Makefile.DragonFly does NOT exist yet. Your diff must CREATE a new file.
You MUST use `--- /dev/null` (not `--- a/...`) as the source."""
    
    parts.append(f"""
## Instructions

Generate a patch that fixes this issue for the DeltaPorts overlay.
{file_instruction}

### Output Format

Output a single unified diff in a ```diff code block. The diff must:

1. Use paths relative to DeltaPorts overlay root: `ports/category/portname/...`
2. Target `Makefile.DragonFly` for DragonFly-specific fixes (preferred)
3. Use `diffs/*.diff` only when patching upstream source files

### Creating New Files (when Makefile.DragonFly does NOT exist)

```diff
--- /dev/null
+++ b/ports/net/example/Makefile.DragonFly
@@ -0,0 +1,1 @@
+OPTIONS_DEFAULT:=	${{OPTIONS_DEFAULT:NFEATURENAME}}
```

CRITICAL for new files:
- First line MUST be `--- /dev/null` (literally /dev/null, not a path)
- Hunk header MUST be `@@ -0,0 +1,N @@` where N = number of lines
- ALL content lines MUST start with `+`
- Do NOT include `-` lines (there's nothing to remove from /dev/null)

### Modifying Existing Files (when Makefile.DragonFly exists)

```diff
--- a/ports/net/example/Makefile.DragonFly
+++ b/ports/net/example/Makefile.DragonFly
@@ -1,2 +1,3 @@
 # Existing line
+NEW_LINE=value
 Another existing line
```

### Common Fix Patterns

- Disable FreeBSD-only option: `OPTIONS_DEFAULT:= ${{OPTIONS_DEFAULT:NOPTIONNAME}}`
- Add plist entry: `PLIST_FILES+= path/to/file`
- Multiple option removals: `OPTIONS_DEFAULT:= ${{OPTIONS_DEFAULT:NOPT1:NOPT2}}`
""".strip())
    
    return "\n".join(parts)


# -----------------------------------------------------------------------------
# API calls
# -----------------------------------------------------------------------------

def call_opencode(
    url: str,
    payload: str,
    agent: str | None,
    provider: str | None,
    model: str | None,
    timeout: int
) -> dict:
    """
    Call opencode serve API:
    1. POST /session to create session
    2. POST /session/<id>/message to send payload
    Returns the full response dict.
    """
    headers = {"Content-Type": "application/json"}
    
    # Create session
    session_url = f"{url.rstrip('/')}/session"
    req = urllib.request.Request(
        session_url,
        data=json.dumps({}).encode(),
        headers=headers,
        method="POST"
    )
    with urllib.request.urlopen(req, timeout=timeout) as resp:
        session_data = json.loads(resp.read().decode())
    
    session_id = session_data.get("id")
    if not session_id:
        raise RuntimeError(f"No session ID in response: {session_data}")
    
    # Send message
    message_url = f"{url.rstrip('/')}/session/{session_id}/message"
    message_body = {
        "parts": [{"type": "text", "text": payload}]
    }
    
    # Agent must be specified in the message body for subagents
    if agent:
        message_body["agent"] = agent
    
    # Add model specification if provided
    if provider and model:
        message_body["model"] = {"providerID": provider, "modelID": model}
    elif model:
        message_body["model"] = {"providerID": "opencode", "modelID": model}
    
    req = urllib.request.Request(
        message_url,
        data=json.dumps(message_body).encode(),
        headers=headers,
        method="POST"
    )
    with urllib.request.urlopen(req, timeout=timeout) as resp:
        response_data = json.loads(resp.read().decode())
    
    response_data["_session_id"] = session_id
    return response_data


def extract_response_text(response: dict) -> str:
    """Extract the assistant's text response from API response."""
    info = response.get("info", {})
    error = info.get("error")
    if error:
        error_name = error.get("name", "UnknownError")
        error_data = error.get("data", {})
        error_msg = error_data.get("message", str(error_data))
        return f"# Error: {error_name}\n\n{error_msg}"
    
    parts = response.get("parts", [])
    text_parts = []
    for part in parts:
        if isinstance(part, dict) and part.get("type") == "text":
            text_parts.append(part.get("text", ""))
        elif isinstance(part, str):
            text_parts.append(part)
    
    if text_parts:
        return "\n".join(text_parts)
    
    return json.dumps(response, indent=2)


# -----------------------------------------------------------------------------
# Output writers
# -----------------------------------------------------------------------------

def write_triage_outputs(bundle_dir: Path, response: dict, session_id: str):
    """Write triage analysis outputs to bundle."""
    analysis_dir = bundle_dir / "analysis"
    analysis_dir.mkdir(exist_ok=True)
    
    with open(analysis_dir / "triage.json", "w") as f:
        json.dump(response, f, indent=2)
    
    with open(analysis_dir / "session_id.txt", "w") as f:
        f.write(session_id + "\n")
    
    text = extract_response_text(response)
    with open(analysis_dir / "triage.md", "w") as f:
        f.write(text + "\n")


def write_patch_outputs(bundle_dir: Path, response: dict, session_id: str) -> tuple[bool, str]:
    """
    Write patch outputs to bundle.
    
    Supports two formats:
    1. New format: FILE content blocks -> generates diff automatically
    2. Legacy format: Agent provides diff directly
    
    Returns (success, error_message).
    """
    analysis_dir = bundle_dir / "analysis"
    analysis_dir.mkdir(exist_ok=True)
    
    # Write raw response
    with open(analysis_dir / "patch.json", "w") as f:
        json.dump(response, f, indent=2)
    
    # Write session ID
    with open(analysis_dir / "patch_session_id.txt", "w") as f:
        f.write(session_id + "\n")
    
    # Extract response text
    text = extract_response_text(response)
    with open(analysis_dir / "patch.md", "w") as f:
        f.write(text + "\n")
    
    # Try new format first: extract FILE content blocks
    files = extract_files_from_response(text)
    
    if files:
        # New format: generate diff from file contents
        diff = generate_combined_diff(bundle_dir, files)
        
        if not diff:
            return False, "No diff generated from file contents (files unchanged?)"
        
        # Write files manifest for debugging
        with open(analysis_dir / "files_manifest.json", "w") as f:
            json.dump({
                "format": "file_content_blocks",
                "files": list(files.keys()),
                "file_count": len(files),
            }, f, indent=2)
        
        # Write individual file contents for debugging
        files_dir = analysis_dir / "files"
        files_dir.mkdir(exist_ok=True)
        for filepath, content in files.items():
            # Convert path to safe filename
            safe_name = filepath.replace("/", "_")
            with open(files_dir / safe_name, "w") as f:
                f.write(content)
        
        # Write the generated diff
        with open(analysis_dir / "patch.diff", "w") as f:
            f.write(diff)
        
        return True, ""
    
    # Fallback: try legacy diff format
    diff = extract_diff_from_response(text)
    if not diff:
        return False, "No FILE blocks or diff found in response"
    
    is_valid, error = validate_diff(diff)
    if not is_valid:
        # Write invalid diff with error note
        with open(analysis_dir / "patch.diff.invalid", "w") as f:
            f.write(f"# VALIDATION ERROR: {error}\n\n")
            f.write(diff)
        return False, f"Invalid diff: {error}"
    
    # Write valid diff
    with open(analysis_dir / "patch.diff", "w") as f:
        f.write(diff + "\n")
    
    return True, ""


# -----------------------------------------------------------------------------
# Job management
# -----------------------------------------------------------------------------

def move_job(job_path: Path, dest_dir: str) -> Path:
    """Move job file to destination directory (done/failed).
    Also moves any associated .job.error file.
    """
    dest = job_path.parent.parent / dest_dir / job_path.name
    job_path.rename(dest)
    
    # Also move error file if it exists
    error_file = job_path.with_suffix(".job.error")
    if error_file.exists():
        error_dest = dest.with_suffix(".job.error")
        try:
            error_file.rename(error_dest)
        except OSError:
            pass  # Best effort
    
    return dest


def write_error_note(job_path: Path, error: str):
    """Write error note next to failed job."""
    error_path = job_path.with_suffix(".job.error")
    with open(error_path, "w") as f:
        f.write(f"timestamp={datetime.now(timezone.utc).isoformat()}\n")
        f.write(f"error={error}\n")


def claim_next_job(queue_root: Path) -> Path | None:
    """Claim the oldest pending job by moving to inflight/."""
    pending_dir = queue_root / "pending"
    inflight_dir = queue_root / "inflight"
    
    jobs = sorted(pending_dir.glob("*.job"))
    
    for job_path in jobs:
        try:
            dest = inflight_dir / job_path.name
            job_path.rename(dest)
            return dest
        except OSError:
            continue
    
    return None


def enqueue_patch_job(queue_root: Path, job: dict):
    """Enqueue a patch job based on completed triage job."""
    ts = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%SZ")
    origin_safe = job.get("origin", "unknown").replace("/", "_")
    pid = os.getpid()
    
    job_name = f"{ts}-{job.get('profile', 'unknown')}-{origin_safe}-{pid}-patch.job"
    
    pending_dir = queue_root / "pending"
    job_path = pending_dir / job_name
    
    content = [
        f"type=patch",
        f"created_ts_utc={ts}",
        f"profile={job.get('profile', '')}",
        f"origin={job.get('origin', '')}",
        f"flavor={job.get('flavor', '')}",
        f"bundle_dir={job.get('bundle_dir', '')}",
        f"run_id={job.get('run_id', '')}",
        f"triage_file={job.get('bundle_dir', '')}/analysis/triage.md",
    ]
    
    # Atomic write
    tmp_path = job_path.with_suffix(".tmp")
    with open(tmp_path, "w") as f:
        f.write("\n".join(content) + "\n")
    tmp_path.rename(job_path)
    
    return job_path


def enqueue_apply_job(queue_root: Path, job: dict, parent_job_name: str) -> Path:
    """Enqueue an apply job based on completed patch job."""
    ts = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%SZ")
    origin_safe = job.get("origin", "unknown").replace("/", "_")
    pid = os.getpid()
    
    job_name = f"{ts}-{job.get('profile', 'unknown')}-{origin_safe}-{pid}-apply.job"
    
    pending_dir = queue_root / "pending"
    job_path = pending_dir / job_name
    
    # Inherit iteration from parent job, or start at 1
    iteration = int(job.get("iteration", "1"))
    max_iterations = int(job.get("max_iterations", str(DEFAULT_MAX_ITERATIONS)))
    
    content = [
        f"type=apply",
        f"created_ts_utc={ts}",
        f"profile={job.get('profile', '')}",
        f"origin={job.get('origin', '')}",
        f"flavor={job.get('flavor', '')}",
        f"bundle_dir={job.get('bundle_dir', '')}",
        f"run_id={job.get('run_id', '')}",
        f"iteration={iteration}",
        f"max_iterations={max_iterations}",
        f"parent_job={parent_job_name}",
    ]
    
    # Include previous_bundle if this is a retry
    previous_bundle = job.get("previous_bundle")
    if previous_bundle:
        content.append(f"previous_bundle={previous_bundle}")
    
    # Atomic write
    tmp_path = job_path.with_suffix(".tmp")
    with open(tmp_path, "w") as f:
        f.write("\n".join(content) + "\n")
    tmp_path.rename(job_path)
    
    return job_path


# -----------------------------------------------------------------------------
# Job processing
# -----------------------------------------------------------------------------

def process_triage_job(
    queue_root: Path,
    job_path: Path,
    job: dict,
    bundle_dir: Path,
    opencode_url: str,
    opencode_provider: str | None,
    opencode_model: str | None,
    timeout: int,
    max_retries: int,
    retry_delay: int,
    kedb_dir: Path | None
) -> tuple[bool, str]:
    """
    Process a triage job.
    Returns (success, status_message).
    Status message can be "done", "snippet_followup", or error text.
    """
    agent = "dports-triage"
    payload = build_triage_payload(bundle_dir, kedb_dir, job)
    snippet_round = int(job.get("snippet_round", "0"))
    origin = job.get("origin", "unknown")
    job_id = job_path.name
    
    last_error = None
    model_info = f"{opencode_provider}/{opencode_model}" if opencode_provider and opencode_model else "agent-default"
    
    for attempt in range(max_retries):
        try:
            activity_log(queue_root, "api_call_start", 
                        f"Calling {agent} for {origin} (attempt {attempt + 1}/{max_retries})",
                        job_id=job_id, extra={"agent": agent, "model": model_info, "round": snippet_round})
            
            start_time = time.time()
            response = call_opencode(opencode_url, payload, agent, opencode_provider, opencode_model, timeout)
            duration_ms = int((time.time() - start_time) * 1000)
            session_id = response.get("_session_id", "unknown")
            
            activity_log(queue_root, "api_call_complete",
                        f"Triage response received for {origin}",
                        job_id=job_id, duration_ms=duration_ms)
            
            write_triage_outputs(bundle_dir, response, session_id)
            activity_log(queue_root, "write_output",
                        f"Wrote triage.md for {origin}",
                        job_id=job_id)
            
            # Check for snippet requests before deciding on patch job
            response_text = extract_response_text(response)
            should_continue, snippet_msg = check_and_handle_snippet_requests(
                queue_root, job, "triage", job_path, bundle_dir, response_text
            )
            
            if not should_continue:
                # Snippet follow-up was enqueued
                return True, "snippet_followup"
            
            # Check if we should auto-enqueue a patch job
            triage = parse_triage_output(bundle_dir / "analysis" / "triage.md")
            if should_enqueue_patch(triage):
                patch_job_path = enqueue_patch_job(queue_root, job)
                activity_log(queue_root, "enqueue_patch",
                            f"Auto-enqueued patch job for {origin} ({triage['classification']})",
                            job_id=job_id, extra={"classification": triage['classification'], "confidence": triage['confidence']})
            else:
                activity_log(queue_root, "no_patch",
                            f"No patch job for {origin} ({triage['classification']})",
                            job_id=job_id, extra={"classification": triage['classification'], "confidence": triage['confidence']})
            
            return True, "done"
            
        except (urllib.error.URLError, urllib.error.HTTPError, OSError, RuntimeError) as e:
            last_error = str(e)
            activity_log(queue_root, "api_error",
                        f"Attempt {attempt + 1} failed: {last_error[:100]}",
                        job_id=job_id)
            
            if attempt < max_retries - 1:
                delay = min(retry_delay * (2 ** attempt), 60)
                log(queue_root, "INFO", f"retrying in {delay}s...")
                time.sleep(delay)
    
    write_error_note(job_path, last_error or "unknown error")
    return False, last_error or "unknown error"


def process_patch_job(
    queue_root: Path,
    job_path: Path,
    job: dict,
    bundle_dir: Path,
    opencode_url: str,
    opencode_provider: str | None,
    opencode_model: str | None,
    timeout: int,
    max_retries: int,
    retry_delay: int,
    kedb_dir: Path | None
) -> tuple[bool, str]:
    """
    Process a patch job.
    Returns (success, status_message).
    Status message can be "done", "snippet_followup", or error text.
    """
    agent = "dports-patch"
    payload = build_patch_payload(bundle_dir, kedb_dir, job)
    snippet_round = int(job.get("snippet_round", "0"))
    origin = job.get("origin", "unknown")
    job_id = job_path.name
    
    last_error = None
    model_info = f"{opencode_provider}/{opencode_model}" if opencode_provider and opencode_model else "agent-default"
    
    for attempt in range(max_retries):
        try:
            activity_log(queue_root, "api_call_start",
                        f"Calling {agent} for {origin} (attempt {attempt + 1}/{max_retries})",
                        job_id=job_id, extra={"agent": agent, "model": model_info, "round": snippet_round})
            
            start_time = time.time()
            response = call_opencode(opencode_url, payload, agent, opencode_provider, opencode_model, timeout)
            duration_ms = int((time.time() - start_time) * 1000)
            session_id = response.get("_session_id", "unknown")
            
            activity_log(queue_root, "api_call_complete",
                        f"Patch response received for {origin}",
                        job_id=job_id, duration_ms=duration_ms)
            
            # Write outputs first (before checking for snippets)
            success, error = write_patch_outputs(bundle_dir, response, session_id)
            
            if success:
                activity_log(queue_root, "write_output",
                            f"Wrote patch.diff for {origin}",
                            job_id=job_id)
            
            # Check for snippet requests
            response_text = extract_response_text(response)
            should_continue, snippet_msg = check_and_handle_snippet_requests(
                queue_root, job, "patch", job_path, bundle_dir, response_text
            )
            
            if not should_continue:
                # Snippet follow-up was enqueued
                activity_log(queue_root, "snippet_followup",
                            f"Enqueued snippet follow-up for {origin}",
                            job_id=job_id, extra={"round": snippet_round + 1})
                return True, "snippet_followup"
            
            # No snippet requests, check if patch was valid
            if success:
                # Auto-enqueue apply job
                apply_job_path = enqueue_apply_job(queue_root, job, job_path.name)
                activity_log(queue_root, "enqueue_apply",
                            f"Auto-enqueued apply job for {origin}",
                            job_id=job_id, extra={"apply_job": apply_job_path.name})
                return True, "done"
            else:
                activity_log(queue_root, "patch_invalid",
                            f"Patch validation failed for {origin}: {error[:80]}",
                            job_id=job_id)
                write_error_note(job_path, error)
                return False, error
            
        except (urllib.error.URLError, urllib.error.HTTPError, OSError, RuntimeError) as e:
            last_error = str(e)
            activity_log(queue_root, "api_error",
                        f"Attempt {attempt + 1} failed: {last_error[:100]}",
                        job_id=job_id)
            
            if attempt < max_retries - 1:
                delay = min(retry_delay * (2 ** attempt), 60)
                log(queue_root, "INFO", f"retrying in {delay}s...")
                time.sleep(delay)
    
    write_error_note(job_path, last_error or "unknown error")
    return False, last_error or "unknown error"


def process_apply_job(
    queue_root: Path,
    job_path: Path,
    job: dict,
    bundle_dir: Path,
) -> tuple[bool, str]:
    """
    Process an apply job by calling apply-patch script.
    Returns (success, status_message).
    
    The apply-patch script handles:
    - Applying the patch to a safe clone
    - Syncing to DPorts and running dsynth rebuild
    - Creating a PR if rebuild succeeds
    """
    origin = job.get("origin", "unknown")
    job_id = job_path.name
    
    # Check iteration limits
    iteration = int(job.get("iteration", "1"))
    max_iterations = int(job.get("max_iterations", str(DEFAULT_MAX_ITERATIONS)))
    
    if iteration > max_iterations:
        activity_log(queue_root, "max_iterations",
                    f"Max iterations ({max_iterations}) reached for {origin}, giving up",
                    job_id=job_id, extra={"iteration": iteration, "max_iterations": max_iterations})
        return False, "max_iterations_reached"
    
    activity_log(queue_root, "apply_start",
                f"Starting apply job for {origin} (iteration {iteration}/{max_iterations})",
                job_id=job_id, extra={"iteration": iteration, "max_iterations": max_iterations})
    
    # Find apply-patch script (same directory as this script)
    script_dir = Path(__file__).resolve().parent
    apply_script = script_dir / "apply-patch"
    
    if not apply_script.exists():
        activity_log(queue_root, "apply_error",
                    f"apply-patch script not found",
                    job_id=job_id)
        return False, "apply-patch script not found"
    
    # Check that patch.diff exists
    patch_file = bundle_dir / "analysis" / "patch.diff"
    if not patch_file.exists():
        activity_log(queue_root, "apply_error",
                    f"patch.diff not found for {origin}",
                    job_id=job_id)
        return False, "patch.diff not found"
    
    # Build command
    apply_flags_raw = os.environ.get("APPLY_PATCH_FLAGS", "--no-push --no-pr")
    apply_flags = shlex.split(apply_flags_raw) if apply_flags_raw.strip() else []
    cmd = [
        str(apply_script),
        "--bundle", str(bundle_dir),
        *apply_flags,
    ]
    
    flags_display = " ".join(apply_flags) if apply_flags else "(none)"
    activity_log(queue_root, "apply_running",
                f"Running apply-patch for {origin} {flags_display}",
                job_id=job_id)
    
    start_time = time.time()
    
    try:
        env = os.environ.copy()
        env["GIT_TERMINAL_PROMPT"] = "0"
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=600,  # 10 minute timeout for rebuild
            env=env,
        )
        
        duration_ms = int((time.time() - start_time) * 1000)
        
        # Log output
        if result.stdout:
            for line in result.stdout.strip().split("\n")[-20:]:  # Last 20 lines
                log(queue_root, "DEBUG", f"apply-patch: {line}")
        if result.stderr:
            for line in result.stderr.strip().split("\n")[-20:]:
                log(queue_root, "DEBUG", f"apply-patch: {line}")
        
        if result.returncode == 0:
            # Success! Check for PR URL
            pr_url_file = bundle_dir / "analysis" / "pr_url.txt"
            pr_url = None
            if pr_url_file.exists():
                pr_url = pr_url_file.read_text().strip()
            
            activity_log(queue_root, "apply_success",
                        f"Apply succeeded for {origin}" + (f" - PR: {pr_url}" if pr_url else ""),
                        job_id=job_id, duration_ms=duration_ms,
                        extra={"pr_url": pr_url} if pr_url else None)
            
            return True, "done"
        else:
            # Rebuild failed - the hook will create a new evidence bundle
            # which will start a new triage cycle (if within iteration limit)
            error_msg = result.stderr[:200] if result.stderr else f"exit code {result.returncode}"
            
            activity_log(queue_root, "apply_failed",
                        f"Apply failed for {origin}: {error_msg[:80]}",
                        job_id=job_id, duration_ms=duration_ms,
                        extra={"iteration": iteration, "returncode": result.returncode})
            
            # Write iteration tracking for the hook to pick up
            runs_root = queue_root.parent / "runs"
            tracking_file = runs_root / ".current_apply_context"
            try:
                tracking_content = [
                    f"previous_bundle={bundle_dir}",
                    f"iteration={iteration}",
                    f"origin={job.get('origin', '')}",
                ]
                tracking_file.write_text("\n".join(tracking_content) + "\n")
                log(queue_root, "DEBUG", f"wrote tracking context to {tracking_file}")
            except OSError as e:
                log(queue_root, "WARN", f"failed to write tracking context: {e}")
            
            return False, error_msg
            
    except subprocess.TimeoutExpired:
        duration_ms = int((time.time() - start_time) * 1000)
        activity_log(queue_root, "apply_timeout",
                    f"Apply timed out for {origin} after 600s",
                    job_id=job_id, duration_ms=duration_ms)
        return False, "timeout"
    except Exception as e:
        activity_log(queue_root, "apply_error",
                    f"Apply error for {origin}: {str(e)[:80]}",
                    job_id=job_id)
        return False, str(e)


def process_job(
    queue_root: Path,
    job_path: Path,
    opencode_url: str,
    opencode_provider: str | None,
    opencode_model: str | None,
    timeout: int,
    max_retries: int,
    retry_delay: int,
    dry_run: bool,
    kedb_dir: Path | None
):
    """Process a single job (dispatch based on type)."""
    job = parse_job_file(job_path)
    job_type = job.get("type", "triage")
    bundle_dir = Path(job.get("bundle_dir", ""))
    snippet_round = job.get("snippet_round", "0")
    origin = job.get("origin", "unknown")
    job_id = job_path.name
    
    log(queue_root, "INFO", f"processing job {job_path.name}")
    
    # Update runner status to show we're working on this job
    update_runner_status("processing", job_id=job_id, stage=f"{job_type}_start",
                        extra={"origin": origin, "type": job_type})
    
    if not bundle_dir.exists():
        log(queue_root, "ERROR", f"bundle_dir does not exist: {bundle_dir}")
        write_error_note(job_path, f"bundle_dir does not exist: {bundle_dir}")
        move_job(job_path, "failed")
        update_runner_status("idle", job_id=None, stage=None)
        return
    
    if dry_run:
        if job_type == "apply":
            log(queue_root, "INFO", f"[dry-run] type=apply, would run apply-patch --bundle {bundle_dir}")
            print("=" * 60)
            print(f"JOB TYPE: apply (iteration={job.get('iteration', '1')})")
            print(f"BUNDLE: {bundle_dir}")
            print(f"ORIGIN: {job.get('origin', 'unknown')}")
            print("=" * 60)
            move_job(job_path, "pending")
            update_runner_status("idle", job_id=None, stage=None)
            return
        elif job_type == "patch":
            payload = build_patch_payload(bundle_dir, kedb_dir, job)
        else:
            payload = build_triage_payload(bundle_dir, kedb_dir, job)
        
        log(queue_root, "INFO", f"[dry-run] type={job_type}, round={snippet_round}, would send payload ({len(payload)} bytes)")
        print("=" * 60)
        print(f"JOB TYPE: {job_type} (snippet_round={snippet_round})")
        print("=" * 60)
        print(payload)
        print("=" * 60)
        move_job(job_path, "pending")
        update_runner_status("idle", job_id=None, stage=None)
        return
    
    # Dispatch based on job type
    if job_type == "apply":
        success, status = process_apply_job(
            queue_root, job_path, job, bundle_dir
        )
    elif job_type == "patch":
        success, status = process_patch_job(
            queue_root, job_path, job, bundle_dir,
            opencode_url, opencode_provider, opencode_model,
            timeout, max_retries, retry_delay, kedb_dir
        )
    else:
        success, status = process_triage_job(
            queue_root, job_path, job, bundle_dir,
            opencode_url, opencode_provider, opencode_model,
            timeout, max_retries, retry_delay, kedb_dir
        )
    
    if success:
        move_job(job_path, "done")
        if status == "snippet_followup":
            log(queue_root, "INFO", "moved job to done/ (snippet follow-up enqueued)")
        else:
            log(queue_root, "INFO", "moved job to done/")
    else:
        # Write error file for visibility in UI
        error_file = job_path.with_suffix(".job.error")
        try:
            error_msg = status[:500] if status else "Unknown error"
            error_file.write_text(error_msg)
            log(queue_root, "DEBUG", f"wrote error file: {error_file}")
        except OSError as e:
            log(queue_root, "WARN", f"failed to write error file: {e}")
        
        move_job(job_path, "failed")
        log(queue_root, "ERROR", f"moved job to failed/ ({status})")
    
    # Update runner status back to idle
    update_runner_status("idle", job_id=None, stage=None)


# -----------------------------------------------------------------------------
# Main
# -----------------------------------------------------------------------------

def main():
    parser = argparse.ArgumentParser(description="Process dsynth failure jobs via opencode serve")
    parser.add_argument("--queue-root", required=True, help="Path to queue directory")
    parser.add_argument("--once", action="store_true", help="Process one job and exit")
    parser.add_argument("--dry-run", action="store_true", help="Print payload without calling opencode")
    parser.add_argument("--kedb-dir", help="Path to KEDB directory (default: auto-detect)")
    args = parser.parse_args()
    
    queue_root = Path(args.queue_root)
    
    # Validate queue structure
    for subdir in ["pending", "inflight", "done", "failed"]:
        d = queue_root / subdir
        if not d.exists():
            print(f"error: queue directory missing: {d}", file=sys.stderr)
            sys.exit(1)
    
    # Initialize state DB for activity logging
    init_state_db(queue_root)
    
    # Start heartbeat thread (updates runner_status.updated_at every 5s)
    start_heartbeat()
    
    # Find KEDB directory
    if args.kedb_dir:
        kedb_dir = Path(args.kedb_dir)
        if not kedb_dir.exists():
            print(f"warning: KEDB directory not found: {kedb_dir}", file=sys.stderr)
            kedb_dir = None
    else:
        kedb_dir = find_kedb_dir()
    
    # Get config from environment
    opencode_url = os.environ.get("OPENCODE_URL")
    if not opencode_url and not args.dry_run:
        print("error: OPENCODE_URL environment variable required", file=sys.stderr)
        sys.exit(1)
    
    opencode_provider = os.environ.get("OPENCODE_PROVIDER", "opencode")
    opencode_model = os.environ.get("OPENCODE_MODEL", "gpt-5-nano")
    timeout = int(os.environ.get("OPENCODE_TIMEOUT", "120"))
    max_retries = int(os.environ.get("OPENCODE_MAX_RETRIES", "3"))
    retry_delay = int(os.environ.get("OPENCODE_RETRY_DELAY", "8"))
    
    model_info = f"{opencode_provider}/{opencode_model}"
    kedb_info = str(kedb_dir) if kedb_dir else "none"
    log(queue_root, "INFO", f"starting runner (once={args.once}, dry_run={args.dry_run}, model={model_info}, kedb={kedb_info})")
    
    # Log startup activity
    activity_log(queue_root, "runner_start", f"Runner started (model={model_info})")
    update_runner_status("idle", job_id=None, stage=None)
    
    try:
        if args.once:
            job = claim_next_job(queue_root)
            if job:
                process_job(
                    queue_root, job, opencode_url,
                    opencode_provider, opencode_model,
                    timeout, max_retries, retry_delay,
                    args.dry_run, kedb_dir
                )
            else:
                log(queue_root, "INFO", "no jobs in queue")
        else:
            while True:
                job = claim_next_job(queue_root)
                if job:
                    process_job(
                        queue_root, job, opencode_url,
                        opencode_provider, opencode_model,
                        timeout, max_retries, retry_delay,
                        args.dry_run, kedb_dir
                    )
                else:
                    # Update status to show we're waiting for jobs
                    update_runner_status("idle", job_id=None, stage="waiting")
                    time.sleep(5)
    except KeyboardInterrupt:
        log(queue_root, "INFO", "shutting down (keyboard interrupt)")
        activity_log(queue_root, "runner_stop", "Runner stopped (keyboard interrupt)")
    finally:
        stop_heartbeat()
        update_runner_status("stopped", job_id=None, stage=None)


if __name__ == "__main__":
    main()
